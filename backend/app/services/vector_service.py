from typing import List, Optional, Dict, Any, Tuple
import os
import logging
import numpy as np
from datetime import datetime, timezone
import threading
import inspect

# ç¦ç”¨tqdmè¿›åº¦æ¡ä»¥é¿å…åœ¨å‘é‡åŒ–è¿‡ç¨‹ä¸­å¡ä½
os.environ['TQDM_DISABLE'] = '1'
try:
    import tqdm
    # å…¨å±€ç¦ç”¨è¿›åº¦æ¡ï¼Œé˜²æ­¢åœ¨æ‰¹å¤„ç†æ—¶å¡ä½
    from functools import partial
    tqdm.tqdm = partial(tqdm.tqdm, disable=True)
except ImportError:
    pass

from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.schema import Document, BaseRetriever
from sqlalchemy.orm import Session

from app.core.config import settings
from app.models.database import VectorIndex, Document as DocumentModel
from app.core.database import get_redis_client

logger = logging.getLogger(__name__)

class CategoryFilteredRetriever(BaseRetriever):
    """
    åŸºäºç±»åˆ«è¿‡æ»¤çš„æ£€ç´¢å™¨
    """
    
    def __init__(self, vector_store: FAISS, category: str, map_function):
        super().__init__()
        self._vector_store = vector_store
        self._category = category
        self._map_function = map_function
    
    @property
    def vector_store(self):
        return self._vector_store
    
    @property
    def category(self):
        return self._category
    
    @property
    def map_function(self):
        return self._map_function
    
    def _get_relevant_documents(self, query: str, *, run_manager=None) -> List[Document]:
        """
        è·å–ç›¸å…³æ–‡æ¡£ï¼Œæ ¹æ®ç±»åˆ«è¿›è¡Œè¿‡æ»¤
        """
        try:
            logger.info(f"ğŸ” CategoryFilteredRetrieverå¼€å§‹æœç´¢: '{query[:50]}{'...' if len(query) > 50 else ''}', ç±»åˆ«: {self.category}")
            # ä»å‘é‡æ•°æ®åº“ä¸­æœç´¢ç›¸å…³æ–‡æ¡£å¹¶è·å–ç›¸ä¼¼åº¦åˆ†æ•°
            all_scored_docs = self.vector_store.similarity_search_with_score(query, k=settings.retrieval_k * settings.category_search_multiplier)
            logger.info(f"ğŸ“Š CategoryFilteredRetrieveråŸå§‹ç»“æœ: {len(all_scored_docs)} ä¸ªæ–‡æ¡£")
            
            similarity_threshold = settings.similarity_threshold
            
            # æ ¹æ®ç±»åˆ«ä¸ç›¸ä¼¼åº¦è¿‡æ»¤æ–‡æ¡£
            filtered_docs = []
            for doc, score in all_scored_docs:
                # å½“å‘é‡åº“è¿”å›çš„æ˜¯è·ç¦»ï¼ˆè¶Šå°è¶Šç›¸ä¼¼ï¼‰æˆ–ç›¸ä¼¼åº¦ï¼ˆè¶Šå¤§è¶Šç›¸ä¼¼ï¼‰æ—¶ï¼Œç®€å•åˆ¤æ–­
                is_relevant = False
                if score is None:
                    is_relevant = True  # è‹¥æ— åˆ†æ•°ä¿¡æ¯åˆ™é»˜è®¤ç›¸å…³
                else:
                    # ç»éªŒåˆ¤æ–­ï¼šè‹¥åˆ†æ•°å¤§äºé˜ˆå€¼åˆ™è®¤ä¸ºç›¸å…³ï¼›è‹¥å­˜å‚¨çš„æ˜¯è·ç¦»ï¼Œå¯ä»¥æ”¹æˆ score <= (1 - similarity_threshold)
                    # è‹¥ score è¶Šå°è¶Šç›¸ä¼¼ï¼ˆè·ç¦»ï¼‰ï¼Œæˆ– score è¶Šå¤§è¶Šç›¸ä¼¼ï¼ˆç›¸ä¼¼åº¦ï¼‰ï¼Œä¸¤ç§æƒ…å†µå‡åšå…¼å®¹åˆ¤æ–­
                    is_relevant = (score >= similarity_threshold) or (score <= (1 - similarity_threshold))
                if not is_relevant:
                    continue
                
                doc_category = doc.metadata.get('category', 'é€šç”¨æ–‡æ¡£')
                doc_title = doc.metadata.get('title', '').lower()
                
                # å¦‚æœæ–‡æ¡£ç±»åˆ«æ˜¯'other'ï¼Œä½¿ç”¨æ™ºèƒ½æ˜ å°„
                if doc_category == 'other':
                    mapped_category = self.map_function(doc_title, doc.page_content)
                    if mapped_category == self.category:
                        filtered_docs.append(doc)
                # å¦‚æœæ–‡æ¡£ç±»åˆ«ç›´æ¥åŒ¹é…
                elif doc_category == self.category:
                    filtered_docs.append(doc)
            
            # é™åˆ¶è¿”å›ç»“æœæ•°é‡
            final_docs = filtered_docs[:10]
            logger.info(f"âœ… CategoryFilteredRetrieverè¿‡æ»¤å®Œæˆ: è¿”å› {len(final_docs)} ä¸ªæ–‡æ¡£")
            return final_docs
            
        except Exception as e:
            logger.error(f"CategoryFilteredRetrieveræ£€ç´¢å¤±è´¥: {e}")
            return []
    
    async def _aget_relevant_documents(self, query: str, *, run_manager=None) -> List[Document]:
        """
        å¼‚æ­¥è·å–ç›¸å…³æ–‡æ¡£
        """
        return self._get_relevant_documents(query, run_manager=run_manager)

class VectorService:
    """å‘é‡åŒ–æœåŠ¡ç±»"""
    
    _instance = None
    _init_lock = threading.Lock()

    def __new__(cls, *args, **kwargs):
        if cls._instance is None:
            with cls._init_lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
                    cls._instance._initialized = False
        return cls._instance

    @classmethod
    def get_instance(cls) -> 'VectorService':
        """è·å–å•ä¾‹å®ä¾‹"""
        if cls._instance is None:
            cls()
        return cls._instance
    
    def __init__(self):
        if getattr(self, "_initialized", False):
            return
        # ç‰ˆæœ¬æŒ‡çº¹ï¼ˆç”¨äºåˆ¤æ–­æ˜¯å¦åŠ è½½äº†æ–°ä»£ç ï¼‰
        self.version_fingerprint = datetime.now().strftime("%Y%m%d-%H%M%S")
        
        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹ - æ ¹æ®é…ç½®é€‰æ‹©æœ¬åœ°æˆ–APIæ¨¡å¼
        self.embedding_mode = settings.embedding_mode
        
        if self.embedding_mode == "api":
            # ä½¿ç”¨ Qwen3 Embedding API
            import requests
            # ä»ç¯å¢ƒå˜é‡è¯»å–APIå¯†é’¥
            self.api_key = os.getenv('DASHSCOPE_API_KEY', settings.embedding_api_key)
            self.api_base_url = settings.embedding_base_url
            self.api_headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            }
            logger.info(f"ğŸ†” VectorService å¯åŠ¨ï¼ŒæŒ‡çº¹: {self.version_fingerprint}ï¼Œä½¿ç”¨APIæ¨¡å¼: {settings.embedding_base_url}")
        else:
            # ä½¿ç”¨æœ¬åœ°åµŒå…¥æ¨¡å‹
            logger.info(f"ğŸ†” VectorService å¯åŠ¨ï¼ŒæŒ‡çº¹: {self.version_fingerprint}ï¼Œä½¿ç”¨æœ¬åœ°æ¨¡å¼: {settings.local_embedding_model}")
        
        # åŒ…è£…ä¸º LangChain å…¼å®¹çš„åµŒå…¥æ¨¡å‹
        self._langchain_embeddings = self._create_langchain_wrapper()
        
        # å‘é‡æ•°æ®åº“è·¯å¾„
        self.vector_db_path = settings.vector_path
        self.ensure_vector_db_directory()
        
        # Rediså®¢æˆ·ç«¯ç”¨äºç¼“å­˜
        self.redis_client = None
        
        # å‘é‡æ•°æ®åº“å®ä¾‹
        self._vector_store = None
        
        # æ ‡è®°å·²åˆå§‹åŒ–ï¼Œé¿å…é‡å¤åˆå§‹åŒ–
        self._initialized = True
    
    def _create_langchain_wrapper(self):
        """åˆ›å»ºLangChainå…¼å®¹çš„åµŒå…¥åŒ…è£…å™¨"""
        from langchain.embeddings.base import Embeddings
        
        if self.embedding_mode == "local":
            # ä½¿ç”¨æœ¬åœ°HuggingFaceåµŒå…¥æ¨¡å‹
            return HuggingFaceEmbeddings(
                model_name=settings.local_embedding_model,
                model_kwargs={'device': settings.local_embedding_device},
                encode_kwargs={'normalize_embeddings': True}
            )
        
        # APIæ¨¡å¼ - åˆ›å»ºAPIåŒ…è£…å™¨
        class Qwen3APIEmbeddingWrapper(Embeddings):
            def __init__(self, api_base_url, api_headers):
                self.api_base_url = api_base_url
                self.api_headers = api_headers
                import requests
                self.requests = requests
            
            def embed_documents(self, texts):
                """åµŒå…¥æ–‡æ¡£åˆ—è¡¨ï¼ˆåˆ†æ‰¹å¤„ç†å¹¶æ‰“å°è¿›åº¦ï¼‰"""
                total = len(texts)
                if total == 0:
                    return []
                logger.info(f"ğŸ”„ å¼€å§‹åµŒå…¥ {total} ä¸ªæ–‡æœ¬ï¼ˆAPIè°ƒç”¨ï¼‰")
                
                # APIè°ƒç”¨æ‰¹å¤„ç†å¤§å°ï¼Œé¿å…å•æ¬¡è¯·æ±‚è¿‡å¤§
                batch_size = 10
                
                all_embeddings = []
                for start in range(0, total, batch_size):
                    end = min(start + batch_size, total)
                    batch = texts[start:end]
                    
                    try:
                        # è°ƒç”¨APIè·å–åµŒå…¥å‘é‡
                        payload = {
                            "model": settings.embedding_model_name,
                            "input": batch,
                            "dimensions": settings.embedding_dimensions,
                            "encoding_format": settings.embedding_encoding_format
                        }
                        
                        response = self.requests.post(
                            f"{self.api_base_url}/embeddings",
                            headers=self.api_headers,
                            json=payload,
                            timeout=30
                        )
                        response.raise_for_status()
                        
                        # æå–åµŒå…¥å‘é‡
                        result = response.json()
                        batch_embeddings = [data["embedding"] for data in result["data"]]
                        all_embeddings.extend(batch_embeddings)
                        
                        logger.info(f"â³ åµŒå…¥è¿›åº¦: {end}/{total}")
                        
                    except Exception as e:
                        logger.error(f"âŒ APIè°ƒç”¨å¤±è´¥ï¼Œæ‰¹æ¬¡ {start}-{end}: {e}")
                        # å¦‚æœæ‰¹å¤„ç†å¤±è´¥ï¼Œå°è¯•é€ä¸ªå¤„ç†
                        for single_text in batch:
                            try:
                                single_payload = {
                                    "model": settings.embedding_model_name,
                                    "input": [single_text],
                                    "dimensions": settings.embedding_dimensions,
                                    "encoding_format": settings.embedding_encoding_format
                                }
                                
                                single_response = self.requests.post(
                                    f"{self.api_base_url}/embeddings",
                                    headers=self.api_headers,
                                    json=single_payload,
                                    timeout=30
                                )
                                single_response.raise_for_status()
                                
                                single_result = single_response.json()
                                all_embeddings.append(single_result["data"][0]["embedding"])
                            except Exception as single_e:
                                logger.error(f"âŒ å•ä¸ªæ–‡æœ¬åµŒå…¥å¤±è´¥: {single_e}")
                                # è¿”å›é›¶å‘é‡ä½œä¸ºfallback
                                all_embeddings.append([0.0] * settings.embedding_dimensions)
                
                logger.info(f"âœ… åµŒå…¥å®Œæˆ: {len(all_embeddings)} ä¸ªå‘é‡")
                
                # éªŒè¯è¿”å›çš„åµŒå…¥æ•°é‡ä¸è¾“å…¥æ–‡æœ¬æ•°é‡åŒ¹é…
                if len(all_embeddings) != total:
                    logger.error(f"âŒ åµŒå…¥æ•°é‡ä¸åŒ¹é…: æœŸæœ› {total}, å®é™… {len(all_embeddings)}")
                    # è¡¥é½ç¼ºå¤±çš„åµŒå…¥å‘é‡
                    while len(all_embeddings) < total:
                        all_embeddings.append([0.0] * settings.embedding_dimensions)
                    # æˆªæ–­å¤šä½™çš„åµŒå…¥å‘é‡
                    all_embeddings = all_embeddings[:total]
                    logger.info(f"ğŸ”§ å·²ä¿®æ­£åµŒå…¥æ•°é‡ä¸º: {len(all_embeddings)}")
                
                return all_embeddings
            
            def embed_query(self, text):
                """åµŒå…¥æŸ¥è¯¢æ–‡æœ¬"""
                try:
                    payload = {
                        "model": settings.embedding_model_name,
                        "input": [text],
                        "dimensions": settings.embedding_dimensions,
                        "encoding_format": settings.embedding_encoding_format
                    }
                    
                    response = self.requests.post(
                        f"{self.api_base_url}/embeddings",
                        headers=self.api_headers,
                        json=payload,
                        timeout=30
                    )
                    response.raise_for_status()
                    
                    result = response.json()
                    return result["data"][0]["embedding"]
                except Exception as e:
                    logger.error(f"âŒ æŸ¥è¯¢æ–‡æœ¬åµŒå…¥å¤±è´¥: {e}")
                    # è¿”å›é›¶å‘é‡ä½œä¸ºfallback
                    return [0.0] * settings.embedding_dimensions
        
        # APIæ¨¡å¼ä¸‹è¿”å›APIåŒ…è£…å™¨
        return Qwen3APIEmbeddingWrapper(self.api_base_url, self.api_headers)
    
    def switch_embedding_mode(self, new_mode: str):
        """åŠ¨æ€åˆ‡æ¢embeddingæ¨¡å¼"""
        if new_mode not in ["api", "local"]:
            raise ValueError("embedding_mode must be 'api' or 'local'")
        
        if new_mode != self.embedding_mode:
            logger.info(f"ğŸ”„ åˆ‡æ¢embeddingæ¨¡å¼: {self.embedding_mode} -> {new_mode}")
            self.embedding_mode = new_mode
            
            # é‡æ–°åˆå§‹åŒ–APIç›¸å…³å±æ€§
            if new_mode == "api":
                import requests
                # ä»ç¯å¢ƒå˜é‡è¯»å–APIå¯†é’¥
                self.api_key = os.getenv('DASHSCOPE_API_KEY', settings.embedding_api_key)
                self.api_base_url = settings.embedding_base_url
                self.api_headers = {
                    "Authorization": f"Bearer {self.api_key}",
                    "Content-Type": "application/json"
                }
            
            # é‡æ–°åˆ›å»ºåµŒå…¥åŒ…è£…å™¨
            self._langchain_embeddings = self._create_langchain_wrapper()
            
            # æ¸…ç©ºå‘é‡å­˜å‚¨ç¼“å­˜ï¼Œå¼ºåˆ¶é‡æ–°åŠ è½½
            self._vector_store = None
            logger.info(f"âœ… embeddingæ¨¡å¼åˆ‡æ¢å®Œæˆ: {new_mode}")
            # é€šçŸ¥å‘é‡åº“ç›¸å…³ç»„ä»¶åˆ·æ–°ï¼ˆè§¦å‘é—®ç­”é“¾é‡å»ºã€æ¸…ç†ç¼“å­˜ï¼‰
            try:
                self._notify_vector_store_updated()
            except Exception as e:
                logger.warning(f"é€šçŸ¥å‘é‡åº“æ›´æ–°äº‹ä»¶å¤±è´¥: {e}")
    
    def _get_vector_path_for_mode(self, mode: str) -> str:
        """æ ¹æ®embeddingæ¨¡å¼è·å–å‘é‡å­˜å‚¨è·¯å¾„"""
        if mode == "api":
            return os.path.join(self.vector_db_path, "api_text_embedding_v4")
        else:  # local mode
            return os.path.join(self.vector_db_path, "local_text_embedding_ada_002")
    
    def ensure_vector_db_directory(self):
        """ç¡®ä¿å‘é‡æ•°æ®åº“ç›®å½•å­˜åœ¨"""
        os.makedirs(self.vector_db_path, exist_ok=True)
    
    def get_redis_client(self):
        """è·å–Rediså®¢æˆ·ç«¯"""
        if self.redis_client is None:
            self.redis_client = get_redis_client()
        return self.redis_client

    def _bump_vector_store_version(self):
        """å¢åŠ å‘é‡åº“ç‰ˆæœ¬å·ï¼ˆç”¨äºé€šçŸ¥å…¶ä»–æœåŠ¡åˆ·æ–°æ£€ç´¢å™¨/ç¼“å­˜ï¼‰"""
        try:
            client = self.get_redis_client()
            if client:
                client.incr("vector_store:version")
        except Exception as e:
            logger.warning(f"æ›´æ–°å‘é‡åº“ç‰ˆæœ¬å·å¤±è´¥: {e}")

    def _clear_qa_answer_cache(self, pattern: str = "qa_answer:*") -> int:
        """æ¸…é™¤QAç­”æ¡ˆç¼“å­˜"""
        try:
            client = self.get_redis_client()
            if client:
                keys = client.keys(pattern)
                if keys:
                    return client.delete(*keys)
            return 0
        except Exception as e:
            logger.warning(f"æ¸…é™¤QAç­”æ¡ˆç¼“å­˜å¤±è´¥: {e}")
            return 0

    def _notify_vector_store_updated(self):
        """å‘é‡åº“æ›´æ–°åé€šçŸ¥ï¼šæå‡ç‰ˆæœ¬å¹¶æ¸…ç†QAç¼“å­˜"""
        self._bump_vector_store_version()
        deleted = self._clear_qa_answer_cache()
        if deleted:
            logger.info(f"å‘é‡åº“æ›´æ–°åå·²æ¸…ç† {deleted} ä¸ªQAç­”æ¡ˆç¼“å­˜")
    
    @property
    def vector_store(self) -> Optional[FAISS]:
        """è·å–å‘é‡æ•°æ®åº“å®ä¾‹"""
        if self._vector_store is None:
            self._vector_store = self.load_vector_store()
        return self._vector_store
    
    def load_vector_store(self) -> Optional[FAISS]:
        """åŠ è½½å‘é‡æ•°æ®åº“"""
        try:
            logger.info("ğŸ”„ å¼€å§‹åŠ è½½å‘é‡æ•°æ®åº“...")
            # æ ¹æ®å½“å‰embeddingæ¨¡å¼è·å–æ­£ç¡®çš„å‘é‡å­˜å‚¨è·¯å¾„
            mode_specific_path = self._get_vector_path_for_mode(self.embedding_mode)
            index_path = os.path.join(mode_specific_path, "faiss_index")
            index_file = os.path.join(index_path, "index.faiss")
            
            logger.info(f"ğŸ“ æ£€æŸ¥å‘é‡æ•°æ®åº“è·¯å¾„: {index_path} (æ¨¡å¼: {self.embedding_mode})")
            
            if os.path.exists(index_file):
                logger.info(f"ğŸ“„ æ‰¾åˆ°å‘é‡ç´¢å¼•æ–‡ä»¶: {index_file}")
                logger.info("âš™ï¸ æ­£åœ¨ååºåˆ—åŒ–å‘é‡æ•°æ®åº“...")
                
                # å…¼å®¹ä¸åŒç‰ˆæœ¬çš„ LangChainï¼šè€ç‰ˆæœ¬æ²¡æœ‰ allow_dangerous_deserialization å‚æ•°
                load_kwargs = {}
                try:
                    sig = inspect.signature(FAISS.load_local)
                    if "allow_dangerous_deserialization" in sig.parameters:
                        load_kwargs["allow_dangerous_deserialization"] = True
                except Exception:
                    # ç­¾åæ£€æŸ¥å¤±è´¥åˆ™ä¸ä¼ è¯¥å‚æ•°
                    pass

                try:
                    vector_store = FAISS.load_local(
                        index_path,
                        self._langchain_embeddings,
                        **load_kwargs,
                    )
                except TypeError as te:
                    # å‘åå…¼å®¹ï¼šå¦‚æœæŠ¥ unexpected keyword argumentï¼Œåˆ™å›é€€ä¸ºä¸å¸¦è¯¥å‚æ•°
                    if "allow_dangerous_deserialization" in str(te):
                        logger.warning("å½“å‰ LangChain ç‰ˆæœ¬ä¸æ”¯æŒ allow_dangerous_deserializationï¼Œè‡ªåŠ¨å›é€€ä¸ºå®‰å…¨åŠ è½½æ¨¡å¼")
                        vector_store = FAISS.load_local(index_path, self._langchain_embeddings)
                    else:
                        raise
                
                # è·å–å‘é‡æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯
                total_vectors = vector_store.index.ntotal
                vector_dimension = vector_store.index.d
                
                # ç»´åº¦ä¸€è‡´æ€§æ£€æŸ¥ï¼šé˜²æ­¢æ—§ç´¢å¼•ä¸æ–°åµŒå…¥æ¨¡å‹ç»´åº¦ä¸ä¸€è‡´
                try:
                    test_dim = len(self._langchain_embeddings.embed_query("test"))
                except Exception:
                    test_dim = None
                if test_dim and test_dim != vector_dimension:
                    logger.warning(
                        f"âš ï¸ å‘é‡ç´¢å¼•ç»´åº¦({vector_dimension})ä¸å½“å‰åµŒå…¥æ¨¡å‹ç»´åº¦({test_dim})ä¸ä¸€è‡´ï¼Œå»ºè®®é‡å»ºç´¢å¼•ï¼›æš‚ä¸åŠ è½½æ—§ç´¢å¼•ä»¥é¿å…è¿è¡Œæ—¶é”™è¯¯"
                    )
                    return None
                
                logger.info(f"âœ… å‘é‡æ•°æ®åº“åŠ è½½æˆåŠŸ")
                logger.info(f"ğŸ“Š å‘é‡æ•°æ®åº“ç»Ÿè®¡: {total_vectors} ä¸ªå‘é‡, ç»´åº¦: {vector_dimension}")
                
                return vector_store
            else:
                logger.info(f"âš ï¸ å‘é‡æ•°æ®åº“ä¸å­˜åœ¨: {index_file}")
                logger.info("ğŸ’¡ å°†åœ¨é¦–æ¬¡æ·»åŠ æ–‡æ¡£æ—¶åˆ›å»ºæ–°çš„å‘é‡æ•°æ®åº“")
                return None
        except Exception as e:
            logger.error(f"âŒ åŠ è½½å‘é‡æ•°æ®åº“å¤±è´¥: {e}")
            return None
    
    def save_vector_store(self, vector_store: FAISS):
        """ä¿å­˜å‘é‡æ•°æ®åº“"""
        try:
            logger.info("ğŸ’¾ å¼€å§‹ä¿å­˜å‘é‡æ•°æ®åº“...")
            # æ ¹æ®å½“å‰embeddingæ¨¡å¼è·å–æ­£ç¡®çš„å‘é‡å­˜å‚¨è·¯å¾„
            mode_specific_path = self._get_vector_path_for_mode(self.embedding_mode)
            index_path = os.path.join(mode_specific_path, "faiss_index")
            
            # è·å–ä¿å­˜å‰çš„ç»Ÿè®¡ä¿¡æ¯
            total_vectors = vector_store.index.ntotal
            vector_dimension = vector_store.index.d
            
            logger.info(f"ğŸ“Š å‡†å¤‡ä¿å­˜: {total_vectors} ä¸ªå‘é‡, ç»´åº¦: {vector_dimension}")
            logger.info(f"ğŸ“ ä¿å­˜è·¯å¾„: {index_path}")
            
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            os.makedirs(index_path, exist_ok=True)
            
            # ä¿å­˜å‘é‡æ•°æ®åº“
            vector_store.save_local(index_path)
            
            # æ›´æ–°å½“å‰å®ä¾‹çš„å‘é‡å­˜å‚¨
            self._vector_store = vector_store
            
            # éªŒè¯ä¿å­˜ç»“æœ
            index_file = os.path.join(index_path, "index.faiss")
            if os.path.exists(index_file):
                file_size = os.path.getsize(index_file)
                logger.info(f"âœ… å‘é‡æ•°æ®åº“ä¿å­˜æˆåŠŸ")
                logger.info(f"ğŸ“„ ç´¢å¼•æ–‡ä»¶å¤§å°: {file_size / 1024 / 1024:.2f} MB")
                
                # å¼ºåˆ¶é‡æ–°åŠ è½½å‘é‡æ•°æ®åº“ä»¥ç¡®ä¿æœ€æ–°çŠ¶æ€
                logger.info("ğŸ”„ é‡æ–°åŠ è½½å‘é‡æ•°æ®åº“ä»¥ç¡®ä¿æœ€æ–°çŠ¶æ€...")
                self._vector_store = None  # æ¸…é™¤ç¼“å­˜
                reloaded_store = self.load_vector_store()  # é‡æ–°åŠ è½½
                if reloaded_store:
                    self._vector_store = reloaded_store
                    logger.info(f"âœ… å‘é‡æ•°æ®åº“é‡æ–°åŠ è½½æˆåŠŸï¼ŒåŒ…å« {reloaded_store.index.ntotal} ä¸ªå‘é‡")
                else:
                    logger.warning("âš ï¸ å‘é‡æ•°æ®åº“é‡æ–°åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨å½“å‰å®ä¾‹")
                    self._vector_store = vector_store

                # é€šçŸ¥ç›¸å…³ç»„ä»¶å‘é‡åº“å·²æ›´æ–°ï¼ˆæå‡ç‰ˆæœ¬å¹¶æ¸…ç†QAç­”æ¡ˆç¼“å­˜ï¼‰
                try:
                    self._notify_vector_store_updated()
                except Exception as e:
                    logger.warning(f"å‘é‡åº“æ›´æ–°é€šçŸ¥å¤±è´¥: {e}")
            else:
                logger.warning("âš ï¸ ä¿å­˜å®Œæˆä½†æœªæ‰¾åˆ°ç´¢å¼•æ–‡ä»¶")
                
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜å‘é‡æ•°æ®åº“å¤±è´¥: {e}")

    def create_embeddings(self, texts: List[str]) -> List[List[float]]:
        """åˆ›å»ºæ–‡æœ¬åµŒå…¥å‘é‡"""
        try:
            logger.info(f"ğŸ§® å¼€å§‹åˆ›å»ºåµŒå…¥å‘é‡: {len(texts)} ä¸ªæ–‡æœ¬ç‰‡æ®µ")
            
            # è®°å½•æ–‡æœ¬ç»Ÿè®¡ä¿¡æ¯
            total_chars = sum(len(text) for text in texts)
            avg_length = total_chars / len(texts) if texts else 0
            
            logger.info(f"ğŸ“ æ–‡æœ¬ç»Ÿè®¡: æ€»å­—ç¬¦æ•° {total_chars}, å¹³å‡é•¿åº¦ {avg_length:.1f}")
            
            # åˆ›å»ºåµŒå…¥å‘é‡ï¼ˆå†…éƒ¨å·²åˆ†æ‰¹ï¼‰
            embeddings = self._langchain_embeddings.embed_documents(texts)
            
            # éªŒè¯ç»“æœ
            if embeddings:
                vector_dim = len(embeddings[0]) if embeddings[0] else 0
                logger.info(f"âœ… åµŒå…¥å‘é‡åˆ›å»ºæˆåŠŸ: {len(embeddings)} ä¸ªå‘é‡, ç»´åº¦: {vector_dim}")
            else:
                logger.warning("âš ï¸ æœªç”Ÿæˆä»»ä½•åµŒå…¥å‘é‡")
                
            return embeddings
            
        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºåµŒå…¥å‘é‡å¤±è´¥: {e}")
            raise

    def create_single_embedding(self, text: str) -> List[float]:
        """åˆ›å»ºå•ä¸ªæ–‡æœ¬çš„åµŒå…¥å‘é‡"""
        try:
            embedding = self._langchain_embeddings.embed_query(text)
            return embedding
        except Exception as e:
            logger.error(f"åˆ›å»ºå•ä¸ªåµŒå…¥å‘é‡å¤±è´¥: {e}")
            raise

    def _build_prefixed_text(self, doc: Document) -> str:
        """æ„é€ ç”¨äºåµŒå…¥çš„å‰ç½®æ‹¼æ¥æ–‡æœ¬ï¼ˆæ ‡é¢˜/ç±»åˆ« + æ­£æ–‡ï¼‰"""
        title = (doc.metadata or {}).get('title') or ''
        category = (doc.metadata or {}).get('category') or ''
        parts = []
        if title:
            parts.append(f"æ ‡é¢˜: {title}")
        if category:
            parts.append(f"ç±»åˆ«: {category}")
        if parts:
            prefix = " | ".join(parts)
            return f"{prefix}\n{doc.page_content}"
        return doc.page_content

    def add_documents_to_vector_store(self, 
                                     documents: List[Document],
                                     document_id: str) -> Tuple[bool, Optional[List[List[float]]]]:
        """å¢é‡å°†æ–°æ–‡æ¡£ç‰‡æ®µæ·»åŠ åˆ°å‘é‡æ•°æ®åº“ï¼ˆåªè®¡ç®—æ–°ç‰‡æ®µåµŒå…¥ï¼Œä¸é‡å»ºç´¢å¼•ï¼‰"""
        try:
            logger.info(f"ğŸ“š å¼€å§‹å¢é‡æ·»åŠ æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“: {document_id}")
            logger.info(f"ğŸ“„ åŸå§‹ç‰‡æ®µæ•°é‡: {len(documents)}")

            # è¿›åº¦ï¼šå‡†å¤‡å…ƒæ•°æ®
            self.update_vectorization_progress(document_id, {
                "document_id": document_id,
                "status": "processing",
                "progress": 20,
                "current_step": "å‡†å¤‡æ–‡æ¡£æ•°æ®",
                "total_steps": 4,
                "current_step_index": 1,
                "message": f"æ­£åœ¨å‡†å¤‡ {len(documents)} ä¸ªæ–‡æ¡£ç‰‡æ®µçš„å…ƒæ•°æ®",
                "error": None
            })

            # ç¡®ä¿æ¯ä¸ªç‰‡æ®µéƒ½å¸¦æœ‰ document_id
            for i, doc in enumerate(documents):
                doc.metadata['document_id'] = document_id
                if i % 1000 == 0:
                    logger.info(f"ğŸ“ å¤„ç†æ–‡æ¡£ç‰‡æ®µè¿›åº¦: {i+1}/{len(documents)}")

            # æ„é€ å‰ç¼€æ–‡æœ¬
            prefixed_docs = [
                Document(page_content=self._build_prefixed_text(doc), metadata=doc.metadata)
                for doc in documents
            ]

            # åŠ è½½ï¼ˆæˆ–æ‡’åŠ è½½ï¼‰ç°æœ‰å‘é‡åº“
            vector_store = self.vector_store  # è§¦å‘åŠ è½½

            # è¿‡æ»¤å·²å­˜åœ¨çš„ç‰‡æ®µï¼ˆæ ¹æ® chunk_id å»é‡ï¼Œåªæ·»åŠ æ–°å—ï¼‰
            existing_chunk_ids: set = set()
            if vector_store is not None:
                try:
                    if hasattr(vector_store, "docstore") and hasattr(vector_store.docstore, "_dict"):
                        for _id, _doc in vector_store.docstore._dict.items():
                            cid = (_doc.metadata or {}).get('chunk_id')
                            if cid:
                                existing_chunk_ids.add(cid)
                except Exception as e:
                    logger.warning(f"âš ï¸ è¯»å–ç°æœ‰ç‰‡æ®µå¤±è´¥ï¼Œè·³è¿‡å»é‡: {e}")

            new_docs: List[Document] = []
            skipped = 0
            for d in prefixed_docs:
                cid = d.metadata.get('chunk_id')
                if cid and cid in existing_chunk_ids:
                    skipped += 1
                    continue
                new_docs.append(d)

            logger.info(f"ğŸ“Š æ–°ç‰‡æ®µæ•°é‡: {len(new_docs)}ï¼ˆè·³è¿‡å·²å­˜åœ¨: {skipped}ï¼‰")

            if len(new_docs) == 0:
                logger.info("âœ… æ²¡æœ‰éœ€è¦æ–°å¢çš„ç‰‡æ®µï¼Œå‘é‡åº“ä¿æŒä¸å˜")
                return True, []

            # è¿›åº¦ï¼šè®¡ç®—æ–°ç‰‡æ®µåµŒå…¥
            self.update_vectorization_progress(document_id, {
                "document_id": document_id,
                "status": "processing",
                "progress": 45,
                "current_step": "è®¡ç®—æ–°ç‰‡æ®µåµŒå…¥",
                "total_steps": 4,
                "current_step_index": 2,
                "message": f"æ­£åœ¨è®¡ç®— {len(new_docs)} ä¸ªæ–°æ–‡æ¡£ç‰‡æ®µçš„åµŒå…¥å‘é‡",
                "error": None
            })

            new_texts = [d.page_content for d in new_docs]
            new_metas = [d.metadata for d in new_docs]
            new_embeddings = self.create_embeddings(new_texts)

            if len(new_embeddings) != len(new_texts):
                raise ValueError(f"æ–°æ–‡æœ¬æ•°é‡({len(new_texts)})ä¸åµŒå…¥æ•°é‡({len(new_embeddings)})ä¸åŒ¹é…")
            if not new_embeddings:
                raise ValueError("æœªç”Ÿæˆä»»ä½•æ–°åµŒå…¥å‘é‡")

            # è¿›åº¦ï¼šå†™å…¥å‘é‡ç´¢å¼•
            self.update_vectorization_progress(document_id, {
                "document_id": document_id,
                "status": "processing",
                "progress": 70,
                "current_step": "å†™å…¥å‘é‡ç´¢å¼•",
                "total_steps": 4,
                "current_step_index": 3,
                "message": f"æ­£åœ¨å°† {len(new_docs)} ä¸ªæ–°ç‰‡æ®µå†™å…¥å‘é‡ç´¢å¼•",
                "error": None
            })

            if vector_store is None:
                # é¦–æ¬¡åˆ›å»ºç´¢å¼•ï¼Œä»…ä½¿ç”¨æ–°ç‰‡æ®µ
                logger.info("ğŸ†• åˆ›å»ºæ–°çš„å‘é‡ç´¢å¼•ï¼ˆä»…åŒ…å«æ–°ç‰‡æ®µï¼‰...")
                text_embedding_pairs = list(zip(new_texts, new_embeddings))
                vector_store = FAISS.from_embeddings(
                    text_embedding_pairs,
                    self._langchain_embeddings,
                    metadatas=new_metas
                )
                logger.info(f"âœ… æ–°ç´¢å¼•åˆ›å»ºæˆåŠŸï¼ŒåŒ…å« {len(new_docs)} ä¸ªç‰‡æ®µ")
            else:
                # å¢é‡è¿½åŠ åµŒå…¥
                logger.info("â• å‘ç°æœ‰ç´¢å¼•å¢é‡è¿½åŠ æ–°ç‰‡æ®µ...")
                text_embedding_pairs = list(zip(new_texts, new_embeddings))
                # ä½¿ç”¨ add_embeddings é¿å…å¯¹æ–°ç‰‡æ®µå†æ¬¡è®¡ç®—åµŒå…¥
                vector_store.add_embeddings(
                    text_embedding_pairs,
                    metadatas=new_metas
                )
                logger.info(f"âœ… è¿½åŠ å®Œæˆï¼Œå½“å‰ç´¢å¼•æ€»å‘é‡æ•°: {vector_store.index.ntotal}")

            # è¿›åº¦ï¼šä¿å­˜å‘é‡åº“
            self.update_vectorization_progress(document_id, {
                "document_id": document_id,
                "status": "processing",
                "progress": 85,
                "current_step": "ä¿å­˜å‘é‡æ•°æ®åº“",
                "total_steps": 4,
                "current_step_index": 4,
                "message": "æ­£åœ¨ä¿å­˜å‘é‡æ•°æ®åº“åˆ°ç£ç›˜",
                "error": None
            })

            # ä¿å­˜å¹¶åˆ·æ–°å†…å­˜ä¸­çš„å¼•ç”¨
            self.save_vector_store(vector_store)

            # è¿”å›æœ¬æ‰¹æ–°æ–‡æ¡£çš„åµŒå…¥
            return True, new_embeddings

        except Exception as e:
            logger.error(f"âŒ å¢é‡æ·»åŠ æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“å¤±è´¥: {e}")
            return False, None


    def update_vector_indices_in_db(self, 
                                   db: Session, 
                                   document_id: str, 
                                   documents: List[Document],
                                   embeddings: Optional[List[List[float]]] = None) -> bool:
        """æ›´æ–°æ•°æ®åº“ä¸­çš„å‘é‡ç´¢å¼•è®°å½•ï¼ˆå¯å¤ç”¨å·²è®¡ç®—çš„åµŒå…¥ï¼‰"""
        try:
            # åˆ›å»º/å¤ç”¨åµŒå…¥å‘é‡
            texts = [self._build_prefixed_text(doc) for doc in documents]
            if embeddings is None:
                embeddings = self.create_embeddings(texts)
            
            # æ›´æ–°æ•°æ®åº“ä¸­çš„å‘é‡ç´¢å¼•
            for i, (doc, embedding) in enumerate(zip(documents, embeddings)):
                chunk_id = doc.metadata.get('chunk_id')
                
                vector_index = db.query(VectorIndex).filter(
                    VectorIndex.document_id == document_id,
                    VectorIndex.chunk_id == chunk_id
                ).first()
                
                if vector_index:
                    vector_index.embedding_vector = embedding
                    vector_index.vector_created_at = datetime.now(timezone.utc)
                    vector_index.status = "vectorized"
            
            db.commit()
            logger.info(f"æ›´æ–° {len(documents)} ä¸ªå‘é‡ç´¢å¼•è®°å½•")
            return True
            
        except Exception as e:
            logger.error(f"æ›´æ–°å‘é‡ç´¢å¼•è®°å½•å¤±è´¥: {e}")
            db.rollback()
            return False

    def vectorize_document(self, 
                          db: Session, 
                          document_id: str, 
                          documents: List[Document]) -> bool:
        """å‘é‡åŒ–æ–‡æ¡£çš„ä¸»è¦æ–¹æ³•"""
        try:
            logger.info(f"ğŸš€ å¼€å§‹å‘é‡åŒ–æ–‡æ¡£: {document_id}")
            logger.info(f"ğŸ“Š æ–‡æ¡£ç»Ÿè®¡: {len(documents)} ä¸ªç‰‡æ®µ")
            
            # è®¡ç®—æ–‡æ¡£æ€»å­—ç¬¦æ•°
            total_chars = sum(len(doc.page_content) for doc in documents)
            logger.info(f"ğŸ“ æ–‡æ¡£å†…å®¹: æ€»è®¡ {total_chars} å­—ç¬¦")
            
            # åˆå§‹åŒ–è¿›åº¦
            self.update_vectorization_progress(document_id, {
                "document_id": document_id,
                "status": "processing",
                "progress": 0,
                "current_step": "å¼€å§‹å‘é‡åŒ–",
                "total_steps": 4,
                "current_step_index": 0,
                "message": "æ­£åœ¨å‡†å¤‡å‘é‡åŒ–ä»»åŠ¡",
                "error": None
            })
            
            # æ­¥éª¤1: æ·»åŠ åˆ°å‘é‡æ•°æ®åº“ï¼ˆå¹¶å–å¾—åµŒå…¥ï¼‰
            logger.info("ğŸ”„ æ­¥éª¤1: æ·»åŠ æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“")
            self.update_vectorization_progress(document_id, {
                "document_id": document_id,
                "status": "processing",
                "progress": 25,
                "current_step": "ç”Ÿæˆå‘é‡åµŒå…¥",
                "total_steps": 4,
                "current_step_index": 1,
                "message": f"æ­£åœ¨å¤„ç† {len(documents)} ä¸ªæ–‡æ¡£ç‰‡æ®µ",
                "error": None
            })
            
            success, embeddings = self.add_documents_to_vector_store(documents, document_id)
            if not success:
                logger.error("âŒ æ­¥éª¤1å¤±è´¥: æ— æ³•æ·»åŠ åˆ°å‘é‡æ•°æ®åº“")
                self.update_vectorization_progress(document_id, {
                    "document_id": document_id,
                    "status": "error",
                    "progress": 25,
                    "current_step": "å‘é‡åŒ–å¤±è´¥",
                    "total_steps": 4,
                    "current_step_index": 1,
                    "message": "å‘é‡æ•°æ®åº“æ·»åŠ å¤±è´¥",
                    "error": "å‘é‡æ•°æ®åº“æ·»åŠ å¤±è´¥"
                })
                return False
            logger.info("âœ… æ­¥éª¤1å®Œæˆ: å‘é‡æ•°æ®åº“æ›´æ–°æˆåŠŸ")
            
            # æ­¥éª¤2: æ›´æ–°æ•°æ®åº“ä¸­çš„å‘é‡ç´¢å¼•ï¼ˆå¤ç”¨åŒæ‰¹åµŒå…¥ï¼‰
            logger.info("ğŸ”„ æ­¥éª¤2: æ›´æ–°æ•°æ®åº“å‘é‡ç´¢å¼•è®°å½•")
            self.update_vectorization_progress(document_id, {
                "document_id": document_id,
                "status": "processing",
                "progress": 50,
                "current_step": "æ›´æ–°æ•°æ®åº“ç´¢å¼•",
                "total_steps": 4,
                "current_step_index": 2,
                "message": "æ­£åœ¨æ›´æ–°å‘é‡ç´¢å¼•è®°å½•",
                "error": None
            })
            
            success = self.update_vector_indices_in_db(db, document_id, documents, embeddings=embeddings)
            if not success:
                logger.error("âŒ æ­¥éª¤2å¤±è´¥: æ— æ³•æ›´æ–°æ•°æ®åº“ç´¢å¼•")
                self.update_vectorization_progress(document_id, {
                    "document_id": document_id,
                    "status": "error",
                    "progress": 50,
                    "current_step": "ç´¢å¼•æ›´æ–°å¤±è´¥",
                    "total_steps": 4,
                    "current_step_index": 2,
                    "message": "æ•°æ®åº“ç´¢å¼•æ›´æ–°å¤±è´¥",
                    "error": "æ•°æ®åº“ç´¢å¼•æ›´æ–°å¤±è´¥"
                })
                return False
            logger.info("âœ… æ­¥éª¤2å®Œæˆ: æ•°æ®åº“ç´¢å¼•æ›´æ–°æˆåŠŸ")
            
            # æ­¥éª¤3: æ›´æ–°æ–‡æ¡£çŠ¶æ€
            logger.info("ğŸ”„ æ­¥éª¤3: æ›´æ–°æ–‡æ¡£çŠ¶æ€")
            self.update_vectorization_progress(document_id, {
                "document_id": document_id,
                "status": "processing",
                "progress": 75,
                "current_step": "æ›´æ–°æ–‡æ¡£çŠ¶æ€",
                "total_steps": 4,
                "current_step_index": 3,
                "message": "æ­£åœ¨æ›´æ–°æ–‡æ¡£çŠ¶æ€",
                "error": None
            })
            
            document = db.query(DocumentModel).filter(
                DocumentModel.id == document_id
            ).first()
            
            if document:
                old_status = document.status
                document.status = "vectorized"
                document.updated_at = datetime.now(timezone.utc)
                db.commit()
                logger.info(f"ğŸ“„ æ–‡æ¡£çŠ¶æ€æ›´æ–°: {old_status} â†’ vectorized")
            else:
                logger.warning(f"âš ï¸ æœªæ‰¾åˆ°æ–‡æ¡£è®°å½•: {document_id}")
            
            # æ­¥éª¤4: ç¼“å­˜å‘é‡åŒ–çŠ¶æ€
            logger.info("ğŸ”„ æ­¥éª¤4: ç¼“å­˜å‘é‡åŒ–çŠ¶æ€")
            self.update_vectorization_progress(document_id, {
                "document_id": document_id,
                "status": "completed",
                "progress": 100,
                "current_step": "å‘é‡åŒ–å®Œæˆ",
                "total_steps": 4,
                "current_step_index": 4,
                "message": f"æˆåŠŸå¤„ç† {len(documents)} ä¸ªç‰‡æ®µï¼Œå…± {total_chars} å­—ç¬¦",
                "error": None
            })
            
            self.cache_vectorization_status(document_id, "completed")
            logger.info("âœ… æ­¥éª¤4å®Œæˆ: çŠ¶æ€ç¼“å­˜æˆåŠŸ")
            
            logger.info(f"ğŸ‰ æ–‡æ¡£å‘é‡åŒ–å®Œæˆ: {document_id}")
            logger.info(f"ğŸ“ˆ å¤„ç†ç»“æœ: {len(documents)} ä¸ªç‰‡æ®µ, {total_chars} å­—ç¬¦")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ–‡æ¡£å‘é‡åŒ–å¤±è´¥ {document_id}: {e}")
            # æ›´æ–°é”™è¯¯çŠ¶æ€
            self.update_vectorization_progress(document_id, {
                "document_id": document_id,
                "status": "error",
                "progress": 0,
                "current_step": "å‘é‡åŒ–å¤±è´¥",
                "total_steps": 4,
                "current_step_index": 0,
                "message": f"å‘é‡åŒ–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}",
                "error": str(e)
            })
            return False

    def search_similar_documents(self, 
                                query: str, 
                                k: int = None,
                                score_threshold: float = None,
                                filter_dict: Optional[Dict[str, Any]] = None,
                                active_kb_ids: Optional[List[str]] = None) -> List[Tuple[Document, float]]:
        """æœç´¢ç›¸ä¼¼æ–‡æ¡£
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            k: è¿”å›æ–‡æ¡£æ•°é‡ï¼Œé»˜è®¤è¯»å– settings.retrieval_k
            score_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œä½äºè¯¥é˜ˆå€¼çš„æ–‡æ¡£å°†è¢«è¿‡æ»¤ï¼ˆæ³¨æ„ï¼šæ­¤å¤„çš„åˆ†æ•°å®šä¹‰ä¸º"ç›¸ä¼¼åº¦"ï¼Œè¶Šå¤§è¶Šç›¸ä¼¼ï¼‰
            filter_dict: é¢å¤–è¿‡æ»¤æ¡ä»¶ï¼ˆä¾‹å¦‚ {"category": "manual"}ï¼‰ï¼Œä¼šç›´æ¥ä¼ é€’ç»™ FAISS çš„ filter å‚æ•°
            active_kb_ids: æ¿€æ´»çš„çŸ¥è¯†åº“IDåˆ—è¡¨ï¼Œç”¨äºé™åˆ¶æœç´¢èŒƒå›´
        Returns:
            (Document, similarity) åˆ—è¡¨ï¼Œåˆ†æ•°ä¸ºç›¸ä¼¼åº¦ï¼ˆcosineï¼Œç›¸ä¼¼åº¦è¶Šå¤§è¶Šç›¸å…³ï¼‰
        """
        try:
            # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é»˜è®¤å€¼
            if k is None:
                k = settings.retrieval_k
            if score_threshold is None:
                score_threshold = settings.similarity_threshold
                
            logger.info(f"ğŸ” å¼€å§‹å‘é‡æœç´¢: '{query[:50]}{'...' if len(query) > 50 else ''}'")
            logger.info(f"ğŸ“Š æœç´¢å‚æ•°: k={k}, similarity_threshold={score_threshold}")
            
            if self.vector_store is None:
                logger.warning("âš ï¸ å‘é‡æ•°æ®åº“æœªåˆå§‹åŒ–")
                return []
            
            # è·å–å‘é‡æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯
            total_vectors = self.vector_store.index.ntotal
            logger.info(f"ğŸ“š å‘é‡æ•°æ®åº“çŠ¶æ€: æ€»è®¡ {total_vectors} ä¸ªå‘é‡")
            
            # å…ˆå¬å›å€™é€‰
            if settings.vector_use_mmr:
                logger.info("ğŸ”„ æ‰§è¡ŒMMRæ£€ç´¢ï¼ˆæœ€å¤§è¾¹é™…ç›¸å…³æ€§ï¼‰...")
                # å¦‚æœæœ‰è¿‡æ»¤æ¡ä»¶ï¼Œéœ€è¦æœç´¢æ›´å¤šç»“æœç„¶åæ‰‹åŠ¨è¿‡æ»¤
                fetch_k = min(total_vectors, max(1000, settings.vector_mmr_fetch_k * settings.filter_search_multiplier)) if filter_dict else settings.vector_mmr_fetch_k
                k_for_mmr = min(total_vectors, max(1000, k * settings.filter_search_multiplier)) if filter_dict else k
                docs = self.vector_store.max_marginal_relevance_search(
                    query,
                    k=k_for_mmr,
                    fetch_k=fetch_k,
                    lambda_mult=settings.vector_mmr_lambda_mult
                )
                # ä¸éMMRåˆ†æ”¯ä¿æŒä¸€è‡´çš„ç»“æ„ï¼š(doc, åŸå§‹è·ç¦»[MMRæ— åˆ™ä¸ºNone])
                docs_with_scores = [(doc, None) for doc in docs]
                logger.info(f"ğŸ“‹ MMRæ£€ç´¢å€™é€‰: {len(docs_with_scores)} ä¸ªæ–‡æ¡£")
            else:
                logger.info("ğŸ”„ æ‰§è¡Œå‘é‡ç›¸ä¼¼åº¦æœç´¢ï¼ˆåŸºç¡€å¬å›ï¼‰...")
                # å¦‚æœæœ‰è¿‡æ»¤æ¡ä»¶ï¼Œéœ€è¦æœç´¢æ›´å¤šç»“æœç„¶åæ‰‹åŠ¨è¿‡æ»¤
                search_k = min(total_vectors, max(1000, k * settings.filter_search_multiplier)) if filter_dict else k
                docs_with_scores = self.vector_store.similarity_search_with_score(
                    query,
                    k=search_k
                )
                logger.info(f"ğŸ“‹ åŸºç¡€å¬å›: {len(docs_with_scores)} ä¸ªæ–‡æ¡£")
            
            # æ‰‹åŠ¨åº”ç”¨è¿‡æ»¤å™¨ï¼ˆåŸºäºmetadataï¼‰
            filters_applied = []
            
            # åº”ç”¨çŸ¥è¯†åº“è¿‡æ»¤
            if active_kb_ids:
                logger.info(f"ğŸ¯ åº”ç”¨çŸ¥è¯†åº“è¿‡æ»¤å™¨: {active_kb_ids}")
                kb_filtered_docs = []
                for doc, raw_score in docs_with_scores:
                    doc_kb_id = doc.metadata.get('kb_id')
                    if doc_kb_id and str(doc_kb_id) in [str(kb_id) for kb_id in active_kb_ids]:
                        kb_filtered_docs.append((doc, raw_score))
                docs_with_scores = kb_filtered_docs
                filters_applied.append(f"çŸ¥è¯†åº“: {len(active_kb_ids)}ä¸ª")
                logger.info(f"ğŸ“‹ çŸ¥è¯†åº“è¿‡æ»¤åå€™é€‰: {len(docs_with_scores)} ä¸ªæ–‡æ¡£")
            
            # åº”ç”¨å…¶ä»–è¿‡æ»¤å™¨
            if filter_dict:
                logger.info(f"ğŸ”„ åº”ç”¨å…¶ä»–è¿‡æ»¤å™¨: {filter_dict}")
                filtered_docs_with_scores = []
                for doc, raw_score in docs_with_scores:
                    match = True
                    for key, value in filter_dict.items():
                        if doc.metadata.get(key) != value:
                            match = False
                            break
                    if match:
                        filtered_docs_with_scores.append((doc, raw_score))
                docs_with_scores = filtered_docs_with_scores
                filters_applied.append(f"å…¶ä»–: {len(filter_dict)}ä¸ªæ¡ä»¶")
                logger.info(f"ğŸ“‹ å…¶ä»–è¿‡æ»¤åå€™é€‰: {len(docs_with_scores)} ä¸ªæ–‡æ¡£")
            
            if filters_applied:
                logger.info(f"âœ… è¿‡æ»¤å™¨åº”ç”¨å®Œæˆ: {', '.join(filters_applied)}")
            
            # é™åˆ¶å€™é€‰æ•°é‡åˆ°åŸå§‹kå€¼
            docs_with_scores = docs_with_scores[:k]
            
            # ç»Ÿä¸€è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆä½¿ç”¨ä¸å…¥åº“ä¸€è‡´çš„å‰ç¼€æ‹¼æ¥æ–‡æœ¬ + å½’ä¸€åŒ–å‘é‡ç‚¹ç§¯ï¼‰
            if not docs_with_scores:
                return []
            
            query_embedding = np.array(self.create_single_embedding(query), dtype=np.float32)
            prefixed_texts = [self._build_prefixed_text(doc) for doc, _ in docs_with_scores]
            doc_embeddings = self._langchain_embeddings.embed_documents(prefixed_texts)
            
            docs_with_similarities: List[Tuple[Document, float]] = []
            for (doc, raw_score), doc_emb in zip(docs_with_scores, doc_embeddings):
                doc_vec = np.array(doc_emb, dtype=np.float32)
                # ç”±äºæˆ‘ä»¬çš„åµŒå…¥å·²åš normalize_embeddings=Trueï¼Œå†…ç§¯å³ä¸ºä½™å¼¦ç›¸ä¼¼åº¦
                similarity = float(np.dot(query_embedding, doc_vec))
                # å°†åˆ†æ•°ä¿¡æ¯å†™å› metadata
                # - æœ€ç»ˆè¯­ä¹‰åˆ†æ•°ï¼šmetadata['score']ï¼ˆè¶Šå¤§è¶Šç›¸ä¼¼ï¼‰
                # - åŸå§‹FAISSè·ç¦»ï¼ˆè‹¥å­˜åœ¨ï¼‰ï¼šmetadata['faiss_distance'] ä»¥ä¾¿æ’æŸ¥é—®é¢˜
                doc.metadata['score'] = similarity
                if raw_score is not None:
                    try:
                        doc.metadata['faiss_distance'] = float(raw_score)
                    except Exception:
                        doc.metadata['faiss_distance'] = None
                else:
                    doc.metadata['faiss_distance'] = None
                docs_with_similarities.append((doc, similarity))
            
            # æŒ‰ç›¸ä¼¼åº¦é™åºæ’åº
            docs_with_similarities.sort(key=lambda x: x[1], reverse=True)
            logger.info(f"åŸå§‹æœç´¢ç»“æœï¼ˆæŒ‰ç›¸ä¼¼åº¦é™åºï¼‰: {len(docs_with_similarities)} ä¸ªæ–‡æ¡£")
            for i, (doc, sim) in enumerate(docs_with_similarities):
                logger.info(f"  - ç»“æœ {i+1}: similarity={sim:.4f}, title='{doc.metadata.get('title', 'N/A')}', filename='{doc.metadata.get('filename', 'N/A')}', doc_id='{doc.metadata.get('document_id', 'N/A')}'")
            
            # åº”ç”¨é˜ˆå€¼è¿‡æ»¤ï¼ˆåŸºäºç›¸ä¼¼åº¦ï¼‰
            logger.info(f"ğŸ”„ åº”ç”¨ç›¸ä¼¼åº¦é˜ˆå€¼è¿‡æ»¤: >= {score_threshold}")
            filtered_docs_with_scores: List[Tuple[Document, float]] = [
                (doc, sim) for doc, sim in docs_with_similarities if sim >= score_threshold
            ]
            
            filtered_count = len(filtered_docs_with_scores)
            filtered_out = len(docs_with_similarities) - filtered_count
            logger.info(f"âœ… åˆæ­¥è¿‡æ»¤åç»“æœ: {filtered_count} ä¸ªç‰‡æ®µ")
            if filtered_out > 0:
                logger.info(f"ğŸš« è¿‡æ»¤æ‰ {filtered_out} ä¸ªä½åˆ†ç‰‡æ®µ")

            # â€”â€” å»é‡ï¼šåŒä¸€æ–‡æ¡£åªä¿ç•™æœ€é«˜ç›¸ä¼¼åº¦ç‰‡æ®µï¼Œä½†ç¡®ä¿è‡³å°‘è¿”å›3ç§ä¸åŒç±»åˆ«çš„æ–‡æ¡£ â€”â€”
            logger.info("ğŸ”„ å»é‡åŒä¸€æ–‡æ¡£ï¼Œä¿ç•™æœ€é«˜ç›¸ä¼¼åº¦ç‰‡æ®µâ€¦")
            doc_best_map: Dict[str, Tuple[Document, float]] = {}
            for doc, sim in filtered_docs_with_scores:
                doc_id = doc.metadata.get("document_id", "unknown")
                if doc_id not in doc_best_map or sim > doc_best_map[doc_id][1]:
                    doc_best_map[doc_id] = (doc, sim)
            unique_docs_with_scores = list(doc_best_map.values())
            
            # æŒ‰ç›¸ä¼¼åº¦é™åºæ’åº
            unique_docs_with_scores.sort(key=lambda x: x[1], reverse=True)
            logger.info(f"ğŸ“‹ å»é‡åå‰©ä½™ {len(unique_docs_with_scores)} ä¸ªæ–‡æ¡£")
            
            # ç¡®ä¿å¤šæ ·æ€§ï¼šå°è¯•ä»ä¸åŒç±»åˆ«ä¸­é€‰æ‹©æ–‡æ¡£
            category_docs: Dict[str, List[Tuple[Document, float]]] = {}
            for doc, sim in unique_docs_with_scores:
                category = doc.metadata.get("category", "unknown")
                if category not in category_docs:
                    category_docs[category] = []
                category_docs[category].append((doc, sim))
            
            for category, docs in category_docs.items():
                logger.info(f"ç±»åˆ« '{category}' æœ‰ {len(docs)} ä¸ªæ–‡æ¡£")
            
            if len(category_docs) > 1:
                diverse_results: List[Tuple[Document, float]] = []
                # é¦–å…ˆä»æ¯ä¸ªç±»åˆ«ä¸­å–æœ€é«˜åˆ†æ–‡æ¡£
                for category, docs in category_docs.items():
                    if docs:
                        diverse_results.append(docs[0])
                # ç„¶åæŒ‰ç›¸ä¼¼åº¦æ’åºï¼Œæ·»åŠ å‰©ä½™æ–‡æ¡£ç›´åˆ°è¾¾åˆ°k
                remaining_slots = k - len(diverse_results)
                if remaining_slots > 0:
                    remaining_docs: List[Tuple[Document, float]] = []
                    for category, docs in category_docs.items():
                        if len(docs) > 1:
                            remaining_docs.extend(docs[1:])
                    remaining_docs.sort(key=lambda x: x[1], reverse=True)
                    diverse_results.extend(remaining_docs[:remaining_slots])
                diverse_results.sort(key=lambda x: x[1], reverse=True)
                logger.info(f"ğŸ“‹ å¤šæ ·åŒ–åè¿”å› {len(diverse_results)} ä¸ªæ–‡æ¡£")
                return diverse_results
            
            logger.info(f"ğŸ“‹ ä»…æœ‰å•ä¸€ç±»åˆ«ï¼Œè¿”å› {min(len(unique_docs_with_scores), k)} ä¸ªæ–‡æ¡£")
            return unique_docs_with_scores[:k]
            
        except Exception as e:
            logger.error(f"âŒ å‘é‡æœç´¢å¤±è´¥: {e}")
            return []
    
    def search_by_document_id(self, 
                             document_id: str, 
                             k: int = 10) -> List[Document]:
        """æ ¹æ®æ–‡æ¡£IDæœç´¢æ–‡æ¡£ç‰‡æ®µ"""
        try:
            if self.vector_store is None:
                return []
            
            # ä½¿ç”¨è¿‡æ»¤å™¨æœç´¢ç‰¹å®šæ–‡æ¡£çš„ç‰‡æ®µ
            filter_dict = {"document_id": document_id}
            
            # è·å–æ‰€æœ‰åŒ¹é…çš„æ–‡æ¡£
            all_docs = self.vector_store.similarity_search(
                "",  # ç©ºæŸ¥è¯¢
                k=k,
                filter=filter_dict
            )
            
            return all_docs
            
        except Exception as e:
            logger.error(f"æ ¹æ®æ–‡æ¡£IDæœç´¢å¤±è´¥: {e}")
            return []
    
    def delete_document_from_vector_store(self, document_id: str) -> bool:
        """ä»å‘é‡æ•°æ®åº“ä¸­åˆ é™¤æ–‡æ¡£"""
        try:
            if self.vector_store is None:
                return True
            
            # FAISSä¸æ”¯æŒç›´æ¥åˆ é™¤ï¼Œéœ€è¦é‡å»ºç´¢å¼•
            # è¿™é‡Œæˆ‘ä»¬æ ‡è®°ä¸ºåˆ é™¤ï¼Œåœ¨é‡å»ºæ—¶æ’é™¤
            self.cache_deleted_document(document_id)
            
            logger.info(f"æ ‡è®°æ–‡æ¡£ä¸ºåˆ é™¤: {document_id}")
            return True
            
        except Exception as e:
            logger.error(f"åˆ é™¤æ–‡æ¡£å¤±è´¥: {e}")
            return False
    
    def rebuild_vector_store(self, db: Session) -> bool:
        """é‡å»ºå‘é‡æ•°æ®åº“ï¼ˆæ’é™¤å·²åˆ é™¤çš„æ–‡æ¡£ï¼‰"""
        try:
            logger.info("ğŸ”„ å¼€å§‹é‡å»ºå‘é‡æ•°æ®åº“")
            
            # è·å–æ‰€æœ‰æœ‰æ•ˆçš„æ–‡æ¡£ç‰‡æ®µ
            logger.info("ğŸ“‹ æŸ¥è¯¢å‘é‡ç´¢å¼•è®°å½•...")
            vector_indices = db.query(VectorIndex).all()

            if not vector_indices:
                logger.info("âš ï¸ æ²¡æœ‰éœ€è¦é‡å»ºçš„å‘é‡æ•°æ®")
                return True

            logger.info(f"ğŸ“Š æ‰¾åˆ° {len(vector_indices)} ä¸ªå‘é‡ç´¢å¼•è®°å½•")

            # è·å–å·²åˆ é™¤çš„æ–‡æ¡£ID
            logger.info("ğŸ—‘ï¸ æ£€æŸ¥å·²åˆ é™¤çš„æ–‡æ¡£...")
            try:
                deleted_docs = self.get_deleted_documents()
                if deleted_docs:
                    logger.info(f"ğŸ“‹ å·²åˆ é™¤æ–‡æ¡£æ•°é‡: {len(deleted_docs)}")
                else:
                    logger.info("âœ… æ²¡æœ‰å·²åˆ é™¤çš„æ–‡æ¡£")
            except Exception as e:
                logger.warning(f"âš ï¸ æ— æ³•è·å–åˆ é™¤æ–‡æ¡£åˆ—è¡¨: {e}")
                deleted_docs = set()  # å¦‚æœRedisä¸å¯ç”¨ï¼Œå‡è®¾æ²¡æœ‰åˆ é™¤çš„æ–‡æ¡£

            # è¿‡æ»¤æ‰å·²åˆ é™¤çš„æ–‡æ¡£
            logger.info("ğŸ”„ è¿‡æ»¤æœ‰æ•ˆçš„å‘é‡ç´¢å¼•...")
            valid_indices = [
                vi for vi in vector_indices 
                if vi.document_id not in deleted_docs
            ]

            filtered_count = len(vector_indices) - len(valid_indices)
            if filtered_count > 0:
                logger.info(f"ğŸš« è¿‡æ»¤æ‰ {filtered_count} ä¸ªå·²åˆ é™¤æ–‡æ¡£çš„ç´¢å¼•")
            
            logger.info(f"âœ… æœ‰æ•ˆå‘é‡ç´¢å¼•: {len(valid_indices)} ä¸ª")

            if not valid_indices:
                # åˆ›å»ºç©ºçš„å‘é‡æ•°æ®åº“
                logger.info("ğŸ—‘ï¸ æ‰€æœ‰æ–‡æ¡£éƒ½å·²åˆ é™¤ï¼Œåˆ›å»ºç©ºå‘é‡æ•°æ®åº“")
                self._vector_store = None
                return True

            # é‡å»ºæ–‡æ¡£åˆ—è¡¨
            logger.info("ğŸ“„ é‡å»ºæ–‡æ¡£åˆ—è¡¨...")
            documents = []
            total_chars = 0
            
            for i, vi in enumerate(valid_indices):
                # ç¡®ä¿metadataæ˜¯å­—å…¸ç±»å‹
                metadata = vi.vector_metadata if vi.vector_metadata else {}
                if not isinstance(metadata, dict):
                    metadata = {}
                
                doc = Document(
                    page_content=vi.chunk_text,
                    metadata=metadata
                )
                documents.append(doc)
                total_chars += len(vi.chunk_text)
                
                if (i + 1) % 100 == 0:
                    logger.info(f"ğŸ“ å·²å¤„ç† {i + 1}/{len(valid_indices)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")

            logger.info(f"ğŸ“š æ–‡æ¡£é‡å»ºå®Œæˆ: {len(documents)} ä¸ªç‰‡æ®µ, æ€»è®¡ {total_chars} å­—ç¬¦")

            # è·å–é‡å»ºå‰çš„ç»Ÿè®¡ä¿¡æ¯
            old_vector_count = 0
            if self._vector_store is not None:
                old_vector_count = self._vector_store.index.ntotal
                logger.info(f"ğŸ“Š é‡å»ºå‰å‘é‡æ•°é‡: {old_vector_count}")

            # ä½¿ç”¨å‰ç¼€æ–‡æœ¬åˆ›å»ºæ–°çš„å‘é‡æ•°æ®åº“
            logger.info("ğŸ”„ åˆ›å»ºæ–°çš„å‘é‡æ•°æ®åº“ï¼ˆä½¿ç”¨å‰ç¼€æ–‡æœ¬ï¼‰...")
            prefixed_docs = [
                Document(page_content=self._build_prefixed_text(doc), metadata=doc.metadata)
                for doc in documents
            ]
            new_vector_store = FAISS.from_documents(prefixed_docs, self._langchain_embeddings)
            
            new_vector_count = new_vector_store.index.ntotal
            vector_dimension = new_vector_store.index.d
            logger.info(f"âœ… æ–°å‘é‡æ•°æ®åº“åˆ›å»ºæˆåŠŸ: {new_vector_count} ä¸ªå‘é‡, ç»´åº¦: {vector_dimension}")

            # ä¿å­˜æ–°çš„å‘é‡æ•°æ®åº“
            logger.info("ğŸ’¾ ä¿å­˜æ–°çš„å‘é‡æ•°æ®åº“...")
            self.save_vector_store(new_vector_store)

            # æ¸…é™¤åˆ é™¤æ ‡è®°
            logger.info("ğŸ§¹ æ¸…é™¤åˆ é™¤æ ‡è®°...")
            self.clear_deleted_documents()
            logger.info("âœ… åˆ é™¤æ ‡è®°å·²æ¸…é™¤")

            logger.info("ğŸ‰ å‘é‡æ•°æ®åº“é‡å»ºå®Œæˆ")
            logger.info(f"ğŸ“ˆ é‡å»ºç»Ÿè®¡:")
            logger.info(f"   - åŸå‘é‡æ•°é‡: {old_vector_count}")
            logger.info(f"   - æ–°å‘é‡æ•°é‡: {new_vector_count}")
            logger.info(f"   - æ–‡æ¡£ç‰‡æ®µæ•°: {len(documents)}")
            logger.info(f"   - æ€»å­—ç¬¦æ•°: {total_chars}")
            
            return True

        except Exception as e:
            logger.error(f"âŒ é‡å»ºå‘é‡æ•°æ®åº“å¤±è´¥: {e}")
            return False
    
    def rebuild_vector_store_from_documents(self, documents: List[Document]) -> bool:
        """ä»æ–‡æ¡£åˆ—è¡¨é‡å»ºå‘é‡æ•°æ®åº“"""
        try:
            if not documents:
                logger.info("æ²¡æœ‰æ–‡æ¡£éœ€è¦é‡å»ºå‘é‡æ•°æ®åº“")
                return True
            
            # ä½¿ç”¨å‰ç¼€æ–‡æœ¬åˆ›å»ºæ–°çš„å‘é‡æ•°æ®åº“
            prefixed_docs = [
                Document(page_content=self._build_prefixed_text(doc), metadata=doc.metadata)
                for doc in documents
            ]
            new_vector_store = FAISS.from_documents(prefixed_docs, self._langchain_embeddings)
            
            # ä¿å­˜æ–°çš„å‘é‡æ•°æ®åº“
            self.save_vector_store(new_vector_store)
            
            logger.info(f"å‘é‡æ•°æ®åº“é‡å»ºå®Œæˆï¼ŒåŒ…å« {len(documents)} ä¸ªæ–‡æ¡£")
            return True
            
        except Exception as e:
            logger.error(f"ä»æ–‡æ¡£åˆ—è¡¨é‡å»ºå‘é‡æ•°æ®åº“å¤±è´¥: {e}")
            return False
    
    def get_category_retriever(self, category: str) -> Optional[Any]:
        """
        ä¸ºæŒ‡å®šç±»åˆ«åˆ›å»ºæ£€ç´¢å™¨
        
        Args:
            category: æ–‡æ¡£ç±»åˆ«
            
        Returns:
            è¯¥ç±»åˆ«çš„æ£€ç´¢å™¨ï¼Œå¦‚æœæ²¡æœ‰è¯¥ç±»åˆ«çš„æ–‡æ¡£åˆ™è¿”å›None
        """
        try:
            if self.vector_store is None:
                logger.warning(f"å‘é‡æ•°æ®åº“æœªåˆå§‹åŒ–ï¼Œæ— æ³•ä¸ºç±»åˆ« '{category}' åˆ›å»ºæ£€ç´¢å™¨")
                return None
            
            # æ£€æŸ¥è¯¥ç±»åˆ«æ˜¯å¦æœ‰æ–‡æ¡£
            test_docs = self.vector_store.similarity_search(
                "æµ‹è¯•", k=settings.retrieval_k * settings.category_check_multiplier  # è·å–æ›´å¤šæ–‡æ¡£ç”¨äºæ£€æŸ¥
            )
            
            # æ™ºèƒ½ç±»åˆ«æ˜ å°„ï¼šåŸºäºæ–‡æ¡£æ ‡é¢˜å’Œå†…å®¹è¿›è¡Œåˆ†ç±»
            category_docs = []
            for doc in test_docs:
                doc_category = doc.metadata.get('category', 'é€šç”¨æ–‡æ¡£')
                doc_title = doc.metadata.get('title', '').lower()
                
                # ç‰¹æ®Šå¤„ç†: å½“è¯·æ±‚ç±»åˆ«ä¸º 'other' æ—¶ï¼Œç›´æ¥åŒ…å«æ ‡æ³¨ä¸º 'other' çš„æ–‡æ¡£
                if category == 'other':
                    if doc_category == 'other':
                        category_docs.append(doc)
                    continue
                
                # å¦‚æœæ–‡æ¡£ç±»åˆ«æ˜¯'other'ï¼Œæ ¹æ®æ ‡é¢˜è¿›è¡Œæ™ºèƒ½æ˜ å°„
                if doc_category == 'other':
                    mapped_category = self._map_document_to_category(doc_title, doc.page_content)
                    if mapped_category == category:
                        category_docs.append(doc)
                # å¦‚æœæ–‡æ¡£ç±»åˆ«ç›´æ¥åŒ¹é…
                elif doc_category == category:
                    category_docs.append(doc)
            
            if not category_docs:
                logger.warning(f"ç±»åˆ« '{category}' æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡æ¡£")
                return None
            
            logger.info(f"ä¸ºç±»åˆ« '{category}' åˆ›å»ºæ£€ç´¢å™¨ï¼Œæ‰¾åˆ° {len(category_docs)} ä¸ªæ–‡æ¡£")
            
            # ä½¿ç”¨å·²å®šä¹‰çš„CategoryFilteredRetrieverç±»
            return CategoryFilteredRetriever(self.vector_store, category, self._map_document_to_category)
            
        except Exception as e:
            logger.error(f"ä¸ºç±»åˆ« '{category}' åˆ›å»ºæ£€ç´¢å™¨å¤±è´¥: {e}")
            return None
    
    def _map_document_to_category(self, title: str, content: str) -> str:
        """
        æ ¹æ®æ–‡æ¡£æ ‡é¢˜å’Œå†…å®¹æ™ºèƒ½æ˜ å°„åˆ°åˆé€‚çš„ç±»åˆ«
        
        Args:
            title: æ–‡æ¡£æ ‡é¢˜
            content: æ–‡æ¡£å†…å®¹
            
        Returns:
            æ˜ å°„çš„ç±»åˆ«åç§°
        """
        title_lower = title.lower()
        content_lower = content.lower()[:500]  # åªæ£€æŸ¥å‰500å­—ç¬¦
        
        # è´¨é‡æ‰‹å†Œç±»åˆ«å…³é”®è¯
        manual_keywords = ['è´¨é‡æ‰‹å†Œ', 'ç®¡ç†åˆ¶åº¦', 'è§„èŒƒ', 'æ ‡å‡†', 'ä½“ç³»', 'æ–¹é’ˆ', 'æ”¿ç­–']
        if any(keyword in title_lower for keyword in manual_keywords):
            return 'manual'
        
        # å¼€å‘ç±»åˆ«å…³é”®è¯
        development_keywords = ['è®¾è®¡å¼€å‘', 'å¼€å‘', 'æŠ€æœ¯', 'ç¼–ç¨‹', 'ç³»ç»Ÿè®¾è®¡', 'è½¯ä»¶', 'ä»£ç ']
        if any(keyword in title_lower for keyword in development_keywords):
            return 'development'
        
        # ç¨‹åºç±»åˆ«å…³é”®è¯
        procedure_keywords = ['ç¨‹åº', 'æµç¨‹', 'æ“ä½œ', 'ä½œä¸šæŒ‡å¯¼', 'å·¥ä½œæµç¨‹', 'æ­¥éª¤']
        if any(keyword in title_lower for keyword in procedure_keywords):
            return 'procedure'
        
        # è®°å½•ç±»åˆ«å…³é”®è¯
        record_keywords = ['è®°å½•', 'è¡¨å•', 'æ¸…å•', 'æŠ¥å‘Š', 'æ¨¡æ¿', 'æ£€æŸ¥']
        if any(keyword in title_lower for keyword in record_keywords):
            return 'record'
        
        # å¦‚æœæ ‡é¢˜æ— æ³•ç¡®å®šï¼Œæ£€æŸ¥å†…å®¹
        if 'å¼€å‘' in content_lower or 'è®¾è®¡' in content_lower:
            return 'development'
        elif 'è´¨é‡' in content_lower or 'ç®¡ç†' in content_lower:
            return 'manual'
        elif 'ç¨‹åº' in content_lower or 'æµç¨‹' in content_lower:
            return 'procedure'
        elif 'è®°å½•' in content_lower or 'è¡¨å•' in content_lower:
            return 'record'
        
        # é»˜è®¤è¿”å›manualç±»åˆ«
        return 'manual'
    
    def get_retriever(self, k: int = 5) -> Optional[Any]:
        """
        è·å–é€šç”¨æ£€ç´¢å™¨
        
        Args:
            k: è¿”å›çš„æ–‡æ¡£æ•°é‡
            
        Returns:
            é€šç”¨æ£€ç´¢å™¨ï¼Œå¦‚æœå‘é‡æ•°æ®åº“æœªåˆå§‹åŒ–åˆ™è¿”å›None
        """
        try:
            if self.vector_store is None:
                logger.warning("å‘é‡æ•°æ®åº“æœªåˆå§‹åŒ–ï¼Œæ— æ³•åˆ›å»ºé€šç”¨æ£€ç´¢å™¨")
                return None
            
            # åˆ›å»ºåŸºç¡€æ£€ç´¢å™¨
            retriever = self.vector_store.as_retriever(
                search_type="similarity",
                search_kwargs={"k": k}
            )
            
            logger.info(f"åˆ›å»ºé€šç”¨æ£€ç´¢å™¨æˆåŠŸï¼Œk={k}")
            return retriever
            
        except Exception as e:
            logger.error(f"åˆ›å»ºé€šç”¨æ£€ç´¢å™¨å¤±è´¥: {e}")
            return None
    
    def get_vector_store_stats(self) -> Dict[str, Any]:
        """è·å–å‘é‡æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯"""
        try:
            stats = {
                'total_vectors': 0,
                'vector_dimension': 0,
                'index_size_mb': 0,
                'last_updated': None
            }
            
            if self.vector_store is not None:
                stats['total_vectors'] = self.vector_store.index.ntotal
                stats['vector_dimension'] = self.vector_store.index.d
            
            # è·å–ç´¢å¼•æ–‡ä»¶å¤§å°
            index_path = os.path.join(self.vector_db_path, "faiss_index.faiss")
            if os.path.exists(index_path):
                file_size = os.path.getsize(index_path)
                stats['index_size_mb'] = round(file_size / (1024 * 1024), 2)
                stats['last_updated'] = datetime.fromtimestamp(
                    os.path.getmtime(index_path)
                ).isoformat()
            
            return stats
            
        except Exception as e:
            logger.error(f"è·å–å‘é‡æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            return {}
    
    # Redisç¼“å­˜ç›¸å…³æ–¹æ³•
    def cache_vectorization_status(self, document_id: str, status: str):
        """ç¼“å­˜å‘é‡åŒ–çŠ¶æ€"""
        try:
            redis_client = self.get_redis_client()
            if redis_client:
                key = f"vectorization_status:{document_id}"
                redis_client.setex(key, 3600, status)  # ç¼“å­˜1å°æ—¶
        except Exception as e:
            logger.warning(f"ç¼“å­˜å‘é‡åŒ–çŠ¶æ€å¤±è´¥: {e}")
    
    def get_vectorization_status(self, document_id: str) -> Optional[str]:
        """è·å–å‘é‡åŒ–çŠ¶æ€"""
        try:
            redis_client = self.get_redis_client()
            if redis_client:
                key = f"vectorization_status:{document_id}"
                status = redis_client.get(key)
                return status.decode() if status else None
        except Exception as e:
            logger.warning(f"è·å–å‘é‡åŒ–çŠ¶æ€å¤±è´¥: {e}")
            return None
    
    def update_vectorization_progress(self, document_id: str, progress_data: dict):
        """æ›´æ–°å‘é‡åŒ–è¿›åº¦"""
        try:
            redis_client = self.get_redis_client()
            if redis_client:
                import json
                key = f"vectorization_progress:{document_id}"
                value = json.dumps(progress_data, ensure_ascii=False)
                redis_client.setex(key, 3600, value)  # ç¼“å­˜1å°æ—¶
        except Exception as e:
            logger.warning(f"æ›´æ–°å‘é‡åŒ–è¿›åº¦å¤±è´¥: {e}")
    
    def get_vectorization_progress(self, document_id: str) -> Optional[dict]:
        """è·å–å‘é‡åŒ–è¿›åº¦"""
        try:
            redis_client = self.get_redis_client()
            if redis_client:
                import json
                key = f"vectorization_progress:{document_id}"
                progress = redis_client.get(key)
                if progress:
                    return json.loads(progress.decode('utf-8'))
            return None
        except Exception as e:
            logger.warning(f"è·å–å‘é‡åŒ–è¿›åº¦å¤±è´¥: {e}")
            return None
    
    def cache_deleted_document(self, document_id: str):
        """ç¼“å­˜å·²åˆ é™¤çš„æ–‡æ¡£ID"""
        try:
            redis_client = self.get_redis_client()
            if redis_client:
                key = "deleted_documents"
                redis_client.sadd(key, document_id)
        except Exception as e:
            logger.warning(f"ç¼“å­˜åˆ é™¤æ–‡æ¡£å¤±è´¥: {e}")
    
    def get_deleted_documents(self) -> set:
        """è·å–å·²åˆ é™¤çš„æ–‡æ¡£IDåˆ—è¡¨"""
        try:
            redis_client = self.get_redis_client()
            if redis_client:
                key = "deleted_documents"
                deleted_docs = redis_client.smembers(key)
                return {doc.decode() for doc in deleted_docs}
            return set()
        except Exception as e:
            logger.warning(f"è·å–åˆ é™¤æ–‡æ¡£åˆ—è¡¨å¤±è´¥: {e}")
            return set()
    
    def clear_deleted_documents(self):
        """æ¸…é™¤å·²åˆ é™¤æ–‡æ¡£çš„ç¼“å­˜"""
        try:
            redis_client = self.get_redis_client()
            if redis_client:
                key = "deleted_documents"
                redis_client.delete(key)
        except Exception as e:
            logger.warning(f"æ¸…é™¤åˆ é™¤æ–‡æ¡£ç¼“å­˜å¤±è´¥: {e}")
    
    def cache_search_result(self, query: str, results: List[Dict[str, Any]]):
        """ç¼“å­˜æœç´¢ç»“æœ"""
        try:
            redis_client = self.get_redis_client()
            if redis_client:
                import json
                key = f"search_result:{hash(query)}"
                value = json.dumps(results, ensure_ascii=False)
                redis_client.setex(key, 300, value)  # ç¼“å­˜5åˆ†é’Ÿ
        except Exception as e:
            logger.warning(f"ç¼“å­˜æœç´¢ç»“æœå¤±è´¥: {e}")
    
    def get_cached_search_result(self, query: str) -> Optional[List[Dict[str, Any]]]:
        """è·å–ç¼“å­˜çš„æœç´¢ç»“æœ"""
        try:
            redis_client = self.get_redis_client()
            if redis_client:
                import json
                key = f"search_result:{hash(query)}"
                cached_result = redis_client.get(key)
                if cached_result:
                    return json.loads(cached_result.decode())
            return None
        except Exception as e:
            logger.warning(f"è·å–ç¼“å­˜æœç´¢ç»“æœå¤±è´¥: {e}")
            return None

# å‘é‡æœåŠ¡å·¥å…·å‡½æ•°
def calculate_similarity(vector1: List[float], vector2: List[float]) -> float:
    """è®¡ç®—ä¸¤ä¸ªå‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦"""
    try:
        v1 = np.array(vector1)
        v2 = np.array(vector2)
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        dot_product = np.dot(v1, v2)
        norm_v1 = np.linalg.norm(v1)
        norm_v2 = np.linalg.norm(v2)
        
        if norm_v1 == 0 or norm_v2 == 0:
            return 0.0
        
        similarity = dot_product / (norm_v1 * norm_v2)
        return float(similarity)
        
    except Exception as e:
        logger.error(f"è®¡ç®—å‘é‡ç›¸ä¼¼åº¦å¤±è´¥: {e}")
        return 0.0

def normalize_vector(vector: List[float]) -> List[float]:
    """å½’ä¸€åŒ–å‘é‡"""
    try:
        v = np.array(vector)
        norm = np.linalg.norm(v)
        if norm == 0:
            return vector
        return (v / norm).tolist()
    except Exception as e:
        logger.error(f"å‘é‡å½’ä¸€åŒ–å¤±è´¥: {e}")
        return vector