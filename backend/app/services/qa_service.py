from typing import List, Optional, Dict, Any, Tuple
import logging
import json
import uuid
from datetime import datetime, timezone

# from langchain_openai import OpenAI
# from langchain_openai import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.schema import Document
from sqlalchemy.orm import Session

from app.core.config import settings
from app.models.database import QALog, User, VectorIndex
from app.models.schemas import (
    QuestionRequest, AnswerResponse, SourceDocument,
    FormattedAnswer, QALogResponse
)
from app.services.vector_service import VectorService
from app.services.advanced_retriever import AdvancedRetrieverService
from app.services.retrieval.unified_retrieval_service import UnifiedRetrievalService
from app.services.retrieval.config_factory import RetrievalConfigFactory
from app.core.database import get_redis_client, get_db
from app.services.document_processor import DocumentProcessor  # æ–°å¢å¯¼å…¥
from app.services.llm_service import LLMService

logger = logging.getLogger(__name__)

class QAService:
    """æ™ºèƒ½é—®ç­”æœåŠ¡ç±»"""
    
    def __init__(self, model_name: Optional[str] = None):
        # ä½¿ç”¨æŒ‡å®šæ¨¡å‹æˆ–é»˜è®¤æ¨¡å‹
        self.current_model = model_name or settings.default_model
        
        # åˆå§‹åŒ–LLMæ¨¡å‹
        self.llm_service = LLMService(self.current_model)
        self.llm = self.llm_service.get_llm()
        
        # å‘é‡æœåŠ¡
        self.vector_service = VectorService.get_instance()
        
        # é«˜çº§æ£€ç´¢å™¨æœåŠ¡ï¼ˆä¿ç•™å…¼å®¹æ€§ï¼‰
        self.advanced_retriever = None
        try:
            self.advanced_retriever = AdvancedRetrieverService(llm=self.llm)
            logger.info("é«˜çº§æ£€ç´¢å™¨æœåŠ¡åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.warning(f"é«˜çº§æ£€ç´¢å™¨æœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")
        

        
        # åˆå§‹åŒ–ç»Ÿä¸€æ£€ç´¢æœåŠ¡
        try:
            config = RetrievalConfigFactory.create_from_settings()
            self.unified_retrieval_service = UnifiedRetrievalService(config)
            logger.info("âœ… ç»Ÿä¸€æ£€ç´¢æœåŠ¡åˆå§‹åŒ–å®Œæˆ")
        except Exception as e:
            logger.error(f"âŒ ç»Ÿä¸€æ£€ç´¢æœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")
            self.unified_retrieval_service = None
        
        # Rediså®¢æˆ·ç«¯
        self.redis_client = None
        
        # é—®ç­”é“¾
        self._qa_chain = None
        # è®°å½•å·²çŸ¥çš„å‘é‡åº“ç‰ˆæœ¬ï¼ˆç”¨äºåœ¨ç‰ˆæœ¬å˜åŒ–æ—¶é‡å»ºé—®ç­”é“¾ï¼‰
        self._vector_store_version_seen = None
        
        # åˆå§‹åŒ–æç¤ºæ¨¡æ¿
        self.setup_prompts()
    
    def _create_llm(self, model_name: str, model_config: Optional[Dict[str, Any]] = None):
        """åˆ›å»ºLLMå®ä¾‹ï¼ˆé€šè¿‡LLMServiceï¼‰"""
        service = LLMService(model_name)
        # å¦‚éœ€è‡ªå®šä¹‰é…ç½®ï¼Œå¯åœ¨LLMService.switch_modelä¸­å¤„ç†
        return service.get_llm()
    
    def switch_model(self, model_name: str, model_config: Optional[Dict[str, Any]] = None) -> bool:
        """åˆ‡æ¢æ¨¡å‹"""
        try:
            available_models = settings.available_models.split(',')
            if model_name not in available_models:
                logger.error(f"æ¨¡å‹ {model_name} ä¸åœ¨å¯ç”¨æ¨¡å‹åˆ—è¡¨ä¸­: {available_models}")
                return False
            
            self.current_model = model_name
            if not hasattr(self, 'llm_service') or self.llm_service is None:
                self.llm_service = LLMService(model_name)
            else:
                self.llm_service.switch_model(model_name, model_config)
            self.llm = self.llm_service.get_llm()
            # é‡ç½®é—®ç­”é“¾ï¼Œä½¿å…¶ä½¿ç”¨æ–°æ¨¡å‹
            self._qa_chain = None
            logger.info(f"å·²åˆ‡æ¢åˆ°æ¨¡å‹: {model_name}")
            return True
        except Exception as e:
            logger.error(f"åˆ‡æ¢æ¨¡å‹å¤±è´¥: {e}")
            return False
    
    def get_available_models(self) -> List[str]:
        """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
        return settings.available_models.split(',')
    
    def get_current_model(self) -> str:
        """è·å–å½“å‰ä½¿ç”¨çš„æ¨¡å‹"""
        return self.current_model
    
    def get_redis_client(self):
        """è·å–Rediså®¢æˆ·ç«¯"""
        if self.redis_client is None:
            self.redis_client = get_redis_client()
        return self.redis_client

    def _get_current_vector_store_version(self) -> int:
        """è·å–å½“å‰å‘é‡åº“ç‰ˆæœ¬å·ï¼ˆæ¥è‡ªRedisï¼‰ï¼Œé»˜è®¤ä¸º0"""
        try:
            client = self.get_redis_client()
            if client:
                value = client.get("vector_store:version")
                if value is None:
                    return 0
                try:
                    return int(value)
                except Exception:
                    try:
                        return int(value.decode())
                    except Exception:
                        return 0
            return 0
        except Exception as e:
            logger.warning(f"è·å–å‘é‡åº“ç‰ˆæœ¬å¤±è´¥: {e}")
            return 0
    
    def setup_prompts(self):
        """è®¾ç½®æç¤ºæ¨¡æ¿"""
        # é€šç”¨çŸ¥è¯†åº“é—®ç­”æç¤ºæ¨¡æ¿
        self.qa_prompt_template = PromptTemplate(
            input_variables=["context", "question"],
            template="""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„çŸ¥è¯†åº“æ™ºèƒ½åŠ©æ‰‹ã€‚è¯·åŸºäºä»¥ä¸‹æä¾›çš„æ–‡æ¡£å†…å®¹ï¼Œå‡†ç¡®å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

é‡è¦æŒ‡å¯¼åŸåˆ™ï¼š
1. ä¸¥æ ¼åŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹å›ç­”ï¼Œä¸è¦æ·»åŠ æ–‡æ¡£ä¸­æ²¡æœ‰çš„ä¿¡æ¯
2. å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜"æ ¹æ®æä¾›çš„æ–‡æ¡£ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯"
3. å›ç­”è¦å‡†ç¡®ã€å…·ä½“ã€æœ‰ç”¨
4. å¦‚æœæ¶‰åŠæ­¥éª¤æˆ–æµç¨‹ï¼Œè¯·æŒ‰é¡ºåºåˆ—å‡º
5. å¦‚æœæœ‰é‡è¦æ³¨æ„äº‹é¡¹ï¼Œè¯·ç‰¹åˆ«å¼ºè°ƒ
6. ä¿æŒä¸“ä¸šå’Œå‹å¥½çš„è¯­è°ƒ
7. å¦‚æœé—®é¢˜æ¶‰åŠåˆ—è¡¨æˆ–åˆ†ç±»ä¿¡æ¯ï¼Œè¯·ä»”ç»†æ•´åˆæ‰€æœ‰æ–‡æ¡£ç‰‡æ®µä¸­çš„ç›¸å…³ä¿¡æ¯ï¼Œç¡®ä¿å›ç­”å®Œæ•´
8. å¯¹äºåˆ—è¡¨ç±»é—®é¢˜ï¼Œè¯·é€ä¸€æ£€æŸ¥æ‰€æœ‰æ–‡æ¡£ç‰‡æ®µï¼Œé¿å…é—æ¼ä»»ä½•é¡¹ç›®

æ–‡æ¡£å†…å®¹ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{question}

è¯·æä¾›è¯¦ç»†ã€å‡†ç¡®çš„ç­”æ¡ˆï¼š
"""
        )
        
        # å¤šè½®å¯¹è¯æç¤ºæ¨¡æ¿
        self.chat_prompt_template = PromptTemplate(
            input_variables=["context", "chat_history", "question"],
            template="""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„çŸ¥è¯†åº“æ™ºèƒ½åŠ©æ‰‹ï¼Œæ­£åœ¨ä¸ç”¨æˆ·è¿›è¡Œå¤šè½®å¯¹è¯ã€‚è¯·åŸºäºæ–‡æ¡£å†…å®¹å’Œå¯¹è¯å†å²ï¼Œå›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

æ–‡æ¡£å†…å®¹ï¼š
{context}

å¯¹è¯å†å²ï¼š
{chat_history}

å½“å‰é—®é¢˜ï¼š{question}

è¯·æä¾›å‡†ç¡®ã€æœ‰ç”¨çš„å›ç­”ï¼š
"""
        )
    
    @property
    def qa_chain(self) -> Optional[RetrievalQA]:
        """è·å–é—®ç­”é“¾"""
        try:
            current_version = self._get_current_vector_store_version()
            if self._vector_store_version_seen is None:
                self._vector_store_version_seen = current_version
            elif current_version != self._vector_store_version_seen:
                logger.info(
                    f"æ£€æµ‹åˆ°å‘é‡åº“ç‰ˆæœ¬å˜åŒ–: {self._vector_store_version_seen} -> {current_version}ï¼Œé‡å»ºé—®ç­”é“¾"
                )
                self._qa_chain = None
                self._vector_store_version_seen = current_version
        except Exception as e:
            logger.warning(f"æ£€æŸ¥å‘é‡åº“ç‰ˆæœ¬å¤±è´¥: {e}")
        if self._qa_chain is None:
            self._qa_chain = self.create_qa_chain()
        return self._qa_chain
    
    def create_qa_chain(self) -> Optional[RetrievalQA]:
        """åˆ›å»ºé—®ç­”é“¾"""
        try:
            if self.vector_service.vector_store is None:
                logger.warning("å‘é‡æ•°æ®åº“æœªåˆå§‹åŒ–ï¼Œæ— æ³•åˆ›å»ºé—®ç­”é“¾")
                return None
            
            # é€‰æ‹©æ£€ç´¢å™¨
            # å¦‚æœæœ‰ç»Ÿä¸€æ£€ç´¢æœåŠ¡ï¼Œä¼˜å…ˆä½¿ç”¨åŸºç¡€æ£€ç´¢å™¨ï¼Œé¿å…ä¸ç»Ÿä¸€æ£€ç´¢æœåŠ¡å†²çª
            if self.unified_retrieval_service:
                # ä½¿ç”¨åŸºç¡€æ£€ç´¢å™¨ï¼Œè®©ç»Ÿä¸€æ£€ç´¢æœåŠ¡å¤„ç†é«˜çº§æ£€ç´¢é€»è¾‘
                retriever = self.vector_service.vector_store.as_retriever(
                    search_type="similarity_score_threshold",
                    search_kwargs={
                        "k": settings.retrieval_k,
                        "score_threshold": 0.1
                    }
                )
                logger.info("ä½¿ç”¨åŸºç¡€æ£€ç´¢å™¨ï¼ˆç»Ÿä¸€æ£€ç´¢æœåŠ¡å¯ç”¨ï¼‰")
            elif self.advanced_retriever:
                # ä½¿ç”¨é«˜çº§æ£€ç´¢å™¨
                try:
                    retriever = self.advanced_retriever.create_ensemble_retriever()
                    logger.info("ä½¿ç”¨é«˜çº§Ensembleæ£€ç´¢å™¨")
                except Exception as e:
                    logger.warning(f"é«˜çº§æ£€ç´¢å™¨å¤±è´¥ï¼Œå›é€€åˆ°åŸºç¡€æ£€ç´¢å™¨: {e}")
                    retriever = self.vector_service.vector_store.as_retriever(
                        search_type="similarity_score_threshold",
                        search_kwargs={
                            "k": settings.retrieval_k,
                            "score_threshold": 0.1
                        }
                    )
            else:
                # ä½¿ç”¨åŸºç¡€æ£€ç´¢å™¨
                retriever = self.vector_service.vector_store.as_retriever(
                    search_type="similarity_score_threshold",
                    search_kwargs={
                        "k": settings.retrieval_k,
                        "score_threshold": 0.1  # é™ä½é˜ˆå€¼ä»¥åŒ…å«æ›´å¤šç›¸å…³æ–‡æ¡£
                    }
                )
                logger.info("ä½¿ç”¨åŸºç¡€æ£€ç´¢å™¨ï¼ˆå¸¦ç›¸ä¼¼åº¦é˜ˆå€¼ï¼‰")
            
            # åˆ›å»ºé—®ç­”é“¾
            qa_chain = RetrievalQA.from_chain_type(
                llm=self.llm,
                chain_type="stuff",
                retriever=retriever,
                chain_type_kwargs={
                    "prompt": self.qa_prompt_template
                },
                return_source_documents=True
            )
            
            logger.info("é—®ç­”é“¾åˆ›å»ºæˆåŠŸ")
            return qa_chain
            
        except Exception as e:
            logger.error(f"è®°å½•é—®ç­”æ—¥å¿—å¤±è´¥: {e}")
            db.rollback()
            return None
    
    def ask_question(self, 
                    db: Session,
                    question: str,
                    user_id: Optional[int] = None,
                    category: Optional[str] = None,
                    session_id: Optional[str] = None,
                    overrides: Optional[Dict[str, Any]] = None,
                    kimi_files: Optional[List[str]] = None,
                    active_kb_ids: Optional[List[uuid.UUID]] = None) -> AnswerResponse:
        """å›ç­”é—®é¢˜"""
        try:
            start_time = datetime.now(timezone.utc)
            logger.info(f"ğŸ¤– å¼€å§‹å¤„ç†é—®ç­”è¯·æ±‚: '{question[:100]}{'...' if len(question) > 100 else ''}'") 
            logger.info(f"ğŸ“‹ è¯·æ±‚å‚æ•° - ç”¨æˆ·ID: {user_id}, ç±»åˆ«: {category}, ä¼šè¯ID: {session_id}")
            
            # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨Kimiæ¨¡å‹ä¸”æœ‰æ–‡ä»¶
            if kimi_files and len(kimi_files) > 0 and 'kimi' in self.current_model.lower():
                logger.info(f"ğŸ” æ£€æµ‹åˆ°Kimiæ¨¡å‹æ–‡ä»¶é—®ç­”: {len(kimi_files)} ä¸ªæ–‡ä»¶")
                return self._handle_kimi_file_question(db, question, kimi_files, user_id, session_id, start_time)
            
            # æ£€æŸ¥ç¼“å­˜
            logger.info("ğŸ” æ­¥éª¤1: æ£€æŸ¥ç­”æ¡ˆç¼“å­˜")
            cached_answer = self.get_cached_answer(question, category)
            if cached_answer:
                logger.info("ğŸ’¾ è¿”å›ç¼“å­˜çš„ç­”æ¡ˆ")
                return self.create_answer_response(
                    question=question,
                    answer=cached_answer['answer'],
                    source_documents=cached_answer['sources'],
                    processing_time=0.1,
                    from_cache=True,
                    session_id=session_id,
                    metadata={
                        "retrieval_method": "cache",
                        "category": category
                    }
                )
            else:
                logger.info("âŒ æœªæ‰¾åˆ°ç¼“å­˜ç­”æ¡ˆï¼Œç»§ç»­å¤„ç†")
            
            # ç‰¹æ®Šå¤„ç†ç›¸å…³æ–¹é—®é¢˜
            logger.info("ğŸ” æ­¥éª¤2: æ£€æŸ¥é—®é¢˜ç±»å‹")
            if self._is_stakeholder_question(question):
                logger.info("ğŸ‘¥ æ£€æµ‹åˆ°ç›¸å…³æ–¹é—®é¢˜ï¼Œä½¿ç”¨ä¸“é—¨å¤„ç†é€»è¾‘")
                return self._handle_stakeholder_question(db, question, user_id, session_id, start_time)
            else:
                logger.info("ğŸ“ æ£€æµ‹åˆ°æ™®é€šé—®é¢˜ï¼Œä½¿ç”¨æ ‡å‡†å¤„ç†é€»è¾‘")
            
            # æ£€æŸ¥é—®ç­”é“¾æ˜¯å¦å¯ç”¨
            logger.info("ğŸ” æ­¥éª¤3: æ£€æŸ¥é—®ç­”é“¾çŠ¶æ€")
            if self.qa_chain is None:
                logger.error("âŒ é—®ç­”é“¾æœªåˆå§‹åŒ–")
                return self.create_error_response(
                    "é—®ç­”ç³»ç»Ÿæš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åå†è¯•"
                )
            logger.info("âœ… é—®ç­”é“¾çŠ¶æ€æ­£å¸¸")
            
            # æ„å»ºæŸ¥è¯¢
            logger.info("ğŸ” æ­¥éª¤4: æ„å»ºå¢å¼ºæŸ¥è¯¢")
            query = self.enhance_query(question, category)
            logger.info(f"ğŸ“ å¢å¼ºæŸ¥è¯¢: '{query[:100]}{'...' if len(query) > 100 else ''}'")
            
            # é€‰æ‹©æ£€ç´¢ç­–ç•¥
            if active_kb_ids:
                logger.info("ğŸ” æ­¥éª¤5: æ£€æµ‹åˆ°æŒ‡å®šçŸ¥è¯†åº“IDï¼Œä¼˜å…ˆèµ°æ”¯æŒçŸ¥è¯†åº“è¿‡æ»¤çš„æ£€ç´¢è·¯å¾„")
                if self.unified_retrieval_service:
                    logger.info("ğŸ” ä½¿ç”¨ç»Ÿä¸€æ£€ç´¢æœåŠ¡ï¼ˆå¸¦çŸ¥è¯†åº“è¿‡æ»¤ï¼‰æ‰§è¡Œé—®ç­”")
                    return self.ask_question_with_unified_retrieval(db, question, query, user_id, session_id, start_time, category, overrides, active_kb_ids)
                else:
                    logger.info("ğŸ” ä½¿ç”¨ä¼ ç»Ÿå‘é‡æœåŠ¡ï¼ˆå¸¦çŸ¥è¯†åº“è¿‡æ»¤ï¼‰æ‰§è¡Œé—®ç­”")
                    # ä½¿ç”¨å‘é‡æœåŠ¡çš„çŸ¥è¯†åº“è¿‡æ»¤åŠŸèƒ½
                    kb_ids_str = [str(kb_id) for kb_id in active_kb_ids]
                    docs_with_scores = self.vector_service.search_similar_documents(
                        query=query,
                        k=settings.retrieval_k,
                        active_kb_ids=kb_ids_str
                    )
                    source_docs = [doc for doc, score in docs_with_scores]
                    
                    # å¦‚æœæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ï¼Œä½¿ç”¨LLMç”Ÿæˆç­”æ¡ˆ
                    if source_docs:
                        context = "\n\n".join([doc.page_content for doc in source_docs])
                        prompt = f"""åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ï¼š

ä¸Šä¸‹æ–‡ï¼š
{context}

é—®é¢˜ï¼š{question}

è¯·æä¾›å‡†ç¡®ã€è¯¦ç»†çš„ç­”æ¡ˆï¼š"""
                        
                        try:
                            answer = self.llm.invoke(prompt).content if hasattr(self.llm.invoke(prompt), 'content') else str(self.llm.invoke(prompt))
                        except Exception as e:
                            logger.error(f"LLMè°ƒç”¨å¤±è´¥: {e}")
                            answer = "æŠ±æ­‰ï¼Œåœ¨ç”Ÿæˆç­”æ¡ˆæ—¶é‡åˆ°äº†é—®é¢˜ã€‚"
                        
                        result = {
                            "result": answer,
                            "source_documents": source_docs
                        }
                    else:
                        logger.warning("åœ¨æŒ‡å®šçŸ¥è¯†åº“ä¸­æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£")
                        result = {
                            "result": "æŠ±æ­‰ï¼Œåœ¨æŒ‡å®šçš„çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯æ¥å›ç­”æ‚¨çš„é—®é¢˜ã€‚",
                            "source_documents": []
                        }
                    # ç»§ç»­åç»­å¤„ç†
                # è·³è¿‡å¤šè·¯ç”±ï¼Œå› ä¸ºå…¶æš‚ä¸æ”¯æŒ active_kb_ids
            elif self.unified_retrieval_service:
                logger.info("ğŸ” æ­¥éª¤5: ä½¿ç”¨ç»Ÿä¸€æ£€ç´¢æœåŠ¡æ‰§è¡Œé—®ç­”")
                return self.ask_question_with_unified_retrieval(db, question, query, user_id, session_id, start_time, category, overrides, active_kb_ids)
            else:
                logger.info("ğŸ” æ­¥éª¤5: ä½¿ç”¨ä¼ ç»Ÿé—®ç­”é“¾æ‰§è¡Œå‘é‡æœç´¢")
                # å¦‚æœæŒ‡å®šäº†çŸ¥è¯†åº“IDï¼Œä½¿ç”¨å‘é‡æœåŠ¡ç›´æ¥æœç´¢å¹¶æ„å»ºç­”æ¡ˆ
                if active_kb_ids:
                    logger.info(f"ğŸ¯ é™åˆ¶æœç´¢èŒƒå›´åˆ°çŸ¥è¯†åº“: {active_kb_ids}")
                    # ä½¿ç”¨å‘é‡æœåŠ¡çš„çŸ¥è¯†åº“è¿‡æ»¤åŠŸèƒ½
                    kb_ids_str = [str(kb_id) for kb_id in active_kb_ids]
                    docs_with_scores = self.vector_service.search_similar_documents(
                        query=query,
                        k=settings.retrieval_k,
                        active_kb_ids=kb_ids_str
                    )
                    source_docs = [doc for doc, score in docs_with_scores]
                    
                    # å¦‚æœæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ï¼Œä½¿ç”¨LLMç”Ÿæˆç­”æ¡ˆ
                    if source_docs:
                        context = "\n\n".join([doc.page_content for doc in source_docs])
                        prompt = f"""åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ï¼š

ä¸Šä¸‹æ–‡ï¼š
{context}

é—®é¢˜ï¼š{question}

è¯·æä¾›å‡†ç¡®ã€è¯¦ç»†çš„ç­”æ¡ˆï¼š"""
                        
                        try:
                            answer = self.llm.invoke(prompt).content if hasattr(self.llm.invoke(prompt), 'content') else str(self.llm.invoke(prompt))
                        except Exception as e:
                            logger.error(f"LLMè°ƒç”¨å¤±è´¥: {e}")
                            answer = "æŠ±æ­‰ï¼Œåœ¨ç”Ÿæˆç­”æ¡ˆæ—¶é‡åˆ°äº†é—®é¢˜ã€‚"
                        
                        result = {
                            "result": answer,
                            "source_documents": source_docs
                        }
                    else:
                        logger.warning("åœ¨æŒ‡å®šçŸ¥è¯†åº“ä¸­æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£")
                        result = {
                            "result": "æŠ±æ­‰ï¼Œåœ¨æŒ‡å®šçš„çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯æ¥å›ç­”æ‚¨çš„é—®é¢˜ã€‚",
                            "source_documents": []
                        }
                else:
                    # ä½¿ç”¨ä¼ ç»Ÿé—®ç­”é“¾
                    result = self.qa_chain({"query": query})
            
            source_docs = result.get("source_documents", [])
            logger.info(f"ğŸ“Š å‘é‡æœç´¢å®Œæˆ: {len(source_docs)} ä¸ªç›¸å…³æ–‡æ¡£")
            
            # è®°å½•æ‰¾åˆ°çš„æ–‡æ¡£è¯¦æƒ…
            for i, doc in enumerate(source_docs[:3]):  # åªè®°å½•å‰3ä¸ª
                doc_id = doc.metadata.get('document_id', 'unknown')
                content_preview = doc.page_content[:80].replace('\n', ' ')
                logger.debug(f"ğŸ“„ æ–‡æ¡£ {i+1}: {doc_id} - '{content_preview}...'")
            
            # å¤„ç†ç»“æœ
            logger.info("ğŸ”„ æ­¥éª¤6: å¤„ç†æœç´¢ç»“æœ")
            answer = result.get("result", "")
            logger.info(f"ğŸ“ åŸå§‹ç­”æ¡ˆé•¿åº¦: {len(answer)} å­—ç¬¦")
            
            # åå¤„ç†ç­”æ¡ˆ
            logger.info("ğŸ”„ æ­¥éª¤7: åå¤„ç†ç­”æ¡ˆ")
            formatted_answer = self.post_process_answer(answer, source_docs)
            logger.info(f"ğŸ“ æ ¼å¼åŒ–ç­”æ¡ˆé•¿åº¦: {len(formatted_answer)} å­—ç¬¦")
            
            # è½¬æ¢æºæ–‡æ¡£
            logger.info("ğŸ”„ æ­¥éª¤8: è½¬æ¢æºæ–‡æ¡£æ ¼å¼")
            source_documents = self.convert_source_documents(source_docs)
            logger.info(f"ğŸ“„ æˆåŠŸè½¬æ¢ {len(source_documents)} ä¸ªæºæ–‡æ¡£")
            
            # è®¡ç®—å¤„ç†æ—¶é—´
            processing_time = (datetime.now(timezone.utc) - start_time).total_seconds()
            logger.info(f"â±ï¸ å½“å‰å¤„ç†æ—¶é—´: {processing_time:.2f}ç§’")
            
            # ç¼“å­˜ç­”æ¡ˆ
            logger.info("ğŸ’¾ æ­¥éª¤9: ç¼“å­˜ç­”æ¡ˆ")
            self.cache_answer(question, category, formatted_answer, source_documents)
            
            # è®°å½•é—®ç­”æ—¥å¿—
            logger.info("ğŸ“ æ­¥éª¤10: è®°å½•é—®ç­”äº¤äº’æ—¥å¿—")
            qa_log = self.log_qa_interaction(
                db=db,
                question=question,
                answer=formatted_answer,
                source_documents=source_documents,
                user_id=user_id,
                session_id=session_id,
                processing_time=processing_time
            )
            
            # åˆ›å»ºå“åº”
            logger.info("ğŸ”„ æ­¥éª¤11: åˆ›å»ºå“åº”å¯¹è±¡")
            response = self.create_answer_response(
                question=question,
                answer=formatted_answer,
                source_documents=source_documents,
                processing_time=processing_time,
                session_id=session_id,
                metadata={
                    "retrieval_method": "legacy_retrieval",
                    "category": category
                },
                qa_log_id=qa_log.id if qa_log else None
            )
            
            logger.info(f"âœ… é—®ç­”å¤„ç†å®Œæˆï¼Œæ€»å¤„ç†æ—¶é—´: {processing_time:.2f}ç§’")
            return response
            
        except Exception as e:
            processing_time = (datetime.now(timezone.utc) - start_time).total_seconds()
            logger.error(f"âŒ é—®ç­”å¤„ç†å¤±è´¥: {e} (è€—æ—¶: {processing_time:.2f}ç§’)")
            return self.create_error_response(f"é—®ç­”å¤±è´¥: {str(e)}")
    
    def ask_question_with_unified_retrieval(
         self,
         db: Session,
         question: str,
         query: str,
         user_id: Optional[int] = None,
         session_id: Optional[str] = None,
         start_time: Optional[datetime] = None,
         category: Optional[str] = None,
         overrides: Optional[Dict[str, Any]] = None,
         active_kb_ids: Optional[List[uuid.UUID]] = None
     ) -> AnswerResponse:
         """
         ä½¿ç”¨ç»Ÿä¸€æ£€ç´¢æœåŠ¡å›ç­”é—®é¢˜
         """
         try:
             if start_time is None:
                 start_time = datetime.now(timezone.utc)
             
             logger.info("ğŸš€ å¼€å§‹ç»Ÿä¸€æ£€ç´¢é—®ç­”")
             
             # ä½¿ç”¨ç»Ÿä¸€æ£€ç´¢æœåŠ¡è·å–ç›¸å…³æ–‡æ¡£
             retrieval_params = overrides.copy() if overrides else {}
             if active_kb_ids:
                 retrieval_params['active_kb_ids'] = active_kb_ids
                 logger.info(f"ğŸ¯ é™åˆ¶æ£€ç´¢èŒƒå›´åˆ°çŸ¥è¯†åº“: {active_kb_ids}")
             
             retrieval_result = self.unified_retrieval_service.retrieve(
                 query=query,
                 category=category,
                 **retrieval_params
             )
             
             # æå–æ£€ç´¢ç»“æœ
             source_docs = retrieval_result.documents
             retrieval_metadata = retrieval_result.metadata or {}
             retrieval_mode = retrieval_metadata.get("retrieval_mode", "unknown")
             processing_time_retrieval = retrieval_result.processing_time or 0
             
             logger.info(f"ğŸ¯ æ£€ç´¢ç­–ç•¥: {retrieval_mode}")
             logger.info(f"ğŸ“„ æ‰¾åˆ° {len(source_docs)} ä¸ªç›¸å…³æ–‡æ¡£")
             logger.info(f"â±ï¸ æ£€ç´¢è€—æ—¶: {processing_time_retrieval:.3f}ç§’")
             
             # ä½¿ç”¨LLMç”Ÿæˆç­”æ¡ˆ
             if source_docs:
                 # æ„å»ºä¸Šä¸‹æ–‡
                 context = "\n\n".join([doc.page_content for doc in source_docs[:5]])
                 
                 # ä½¿ç”¨é—®ç­”é“¾ç”Ÿæˆç­”æ¡ˆ
                 qa_input = {
                     "query": query,
                     "context": context
                 }
                 
                 # å¦‚æœæœ‰é—®ç­”é“¾ï¼Œä½¿ç”¨å®ƒç”Ÿæˆç­”æ¡ˆ
                 if self.qa_chain:
                     # ä¸´æ—¶è®¾ç½®æ£€ç´¢å™¨ä¸ºè¿”å›å›ºå®šæ–‡æ¡£çš„æ£€ç´¢å™¨
                     original_retriever = self.qa_chain.retriever
                     
                     class FixedRetriever:
                        def get_relevant_documents(self, query, *, callbacks=None, **kwargs):
                            # å…¼å®¹LangChain Retrievalæ¥å£å¯èƒ½ä¼ å…¥çš„callbacksæˆ–å…¶ä»–å¯é€‰å‚æ•°
                            return source_docs

                        async def aget_relevant_documents(self, query, *, callbacks=None, **kwargs):
                            # æä¾›å¼‚æ­¥æ¥å£ä»¥å…¼å®¹å¯èƒ½çš„å¼‚æ­¥è°ƒç”¨
                            return source_docs
                    
                     self.qa_chain.retriever = FixedRetriever()
                     result = self.qa_chain({"query": query})
                     answer = result.get("result", "")
                     
                     # æ¢å¤åŸå§‹æ£€ç´¢å™¨
                     self.qa_chain.retriever = original_retriever
                 else:
                     # ç›´æ¥ä½¿ç”¨LLMç”Ÿæˆç­”æ¡ˆ
                     prompt = f"""åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ï¼š

ä¸Šä¸‹æ–‡ï¼š
{context}

é—®é¢˜ï¼š{query}

è¯·æä¾›å‡†ç¡®ã€è¯¦ç»†çš„ç­”æ¡ˆï¼š"""
                     answer = self.llm.predict(prompt)
             else:
                 answer = "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯æ¥å›ç­”æ‚¨çš„é—®é¢˜ã€‚"
             
             # åå¤„ç†ç­”æ¡ˆ
             formatted_answer = self.post_process_answer(answer, source_docs)
             
             # è½¬æ¢æºæ–‡æ¡£æ ¼å¼
             source_documents = self.convert_source_documents(source_docs)
             
             # è®¡ç®—æ€»å¤„ç†æ—¶é—´
             total_processing_time = (datetime.now(timezone.utc) - start_time).total_seconds()
             
             # ç¼“å­˜ç­”æ¡ˆ
             self.cache_answer(question, category, formatted_answer, source_documents)
             
             # å…ˆåˆ›å»ºQAæ—¥å¿—è®°å½•ï¼Œä½†ä¸æäº¤
             qa_log = None
             try:
                 # åºåˆ—åŒ–source_documentsï¼Œç¡®ä¿UUIDå¯¹è±¡è¢«è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                 def serialize_value(value):
                     """é€’å½’åºåˆ—åŒ–å€¼ï¼Œç¡®ä¿UUIDå’Œå…¶ä»–éJSONå…¼å®¹ç±»å‹è¢«è½¬æ¢ä¸ºå­—ç¬¦ä¸²"""
                     if isinstance(value, uuid.UUID):
                         return str(value)
                     elif isinstance(value, dict):
                         return {k: serialize_value(v) for k, v in value.items()}
                     elif isinstance(value, list):
                         return [serialize_value(item) for item in value]
                     elif hasattr(value, '__dict__'):
                         # å¯¹äºå¤æ‚å¯¹è±¡ï¼Œå°è¯•è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                         return str(value)
                     else:
                         return value
                 
                 serialized_docs = []
                 for doc in source_documents:
                     doc_dict = doc.dict()
                     # é€’å½’åºåˆ—åŒ–æ•´ä¸ªæ–‡æ¡£å­—å…¸
                     serialized_doc = serialize_value(doc_dict)
                     serialized_docs.append(serialized_doc)
                 
                 # åˆ›å»ºåŒ…å«æ£€ç´¢ä¿¡æ¯çš„å…ƒæ•°æ®
                 log_metadata = {
                     'model': settings.llm_model,
                     'temperature': settings.llm_temperature,
                     'retrieval_k': settings.retrieval_k,
                     'timestamp': datetime.now(timezone.utc).isoformat(),
                     "retrieval_info": {
                        "mode": retrieval_mode,
                        "retrieval_time": processing_time_retrieval,
                        "documents_found": len(source_docs),
                        "auto_selected": retrieval_metadata.get("auto_selected", False)
                    }
                 }
                 
                 qa_log = QALog(
                     question=question,
                     answer=formatted_answer,
                     user_id=user_id,
                     session_id=session_id,
                     retrieved_documents=serialized_docs,
                     response_time=total_processing_time,
                     log_metadata=log_metadata
                 )
                 
                 db.add(qa_log)
                 db.flush()  # åˆ·æ–°ä»¥è·å–IDï¼Œä½†ä¸æäº¤äº‹åŠ¡
                 
                 logger.info(f"QAæ—¥å¿—å·²æ·»åŠ åˆ°ä¼šè¯ï¼ŒID: {qa_log.id}")
                 
             except Exception as e:
                 logger.error(f"è®°å½•é—®ç­”æ—¥å¿—å¤±è´¥: {e}")
                 qa_log = None
                 # ä¸åœ¨è¿™é‡Œå¤„ç†å›æ»šï¼Œè®©APIå±‚ç»Ÿä¸€å¤„ç†äº‹åŠ¡
             
             # åˆ›å»ºå“åº”
             response = self.create_answer_response(
                 question=question,
                 answer=formatted_answer,
                 source_documents=source_documents,
                 processing_time=total_processing_time,
                 session_id=session_id,
                 metadata={
                     "retrieval_mode": retrieval_mode,
                     "retrieval_method": "unified_retrieval",
                     "retrieval_time": processing_time_retrieval,
                     "auto_selected": retrieval_metadata.get("auto_selected", False),
                     "category": category
                 },
                 qa_log_id=qa_log.id if qa_log else None
             )
             
             logger.info(f"âœ… ç»Ÿä¸€æ£€ç´¢é—®ç­”å®Œæˆï¼Œæ€»å¤„ç†æ—¶é—´: {total_processing_time:.2f}ç§’")
             return response
             
         except Exception as e:
             processing_time = (datetime.now(timezone.utc) - start_time).total_seconds() if start_time else 0
             logger.error(f"âŒ ç»Ÿä¸€æ£€ç´¢é—®ç­”å¤±è´¥: {e} (è€—æ—¶: {processing_time:.2f}ç§’)")
             return self.create_error_response(f"ç»Ÿä¸€æ£€ç´¢é—®ç­”å¤±è´¥: {str(e)}")
    

    def enhance_query(self, question: str, category: Optional[str] = None) -> str:
        """å¢å¼ºæŸ¥è¯¢"""
        enhanced_query = question
        
        # æ·»åŠ ç±»åˆ«ä¿¡æ¯
        if category:
            enhanced_query = f"[{category}] {question}"
        
        # æ·»åŠ é€šç”¨å…³é”®è¯å¢å¼º
        general_keywords = ["æ­¥éª¤", "æµç¨‹", "æ–¹æ³•", "æ ‡å‡†", "è§„èŒƒ", "æŒ‡å—"]
        if not any(keyword in question for keyword in general_keywords):
            enhanced_query = f"ç›¸å…³ä¿¡æ¯ï¼š{enhanced_query}"
        
        return enhanced_query
    
    def post_process_answer(self, answer: str, source_docs: List[Document]) -> str:
        """åå¤„ç†ç­”æ¡ˆ"""
        try:
            # æ¸…ç†ç­”æ¡ˆæ ¼å¼
            answer = answer.strip()
            
            # æ·»åŠ ç½®ä¿¡åº¦ä¿¡æ¯
            if len(source_docs) == 0:
                answer += "\n\nâš ï¸ æ³¨æ„ï¼šæ­¤å›ç­”åŸºäºæœ‰é™çš„æ–‡æ¡£ä¿¡æ¯ï¼Œå»ºè®®è¿›ä¸€æ­¥ç¡®è®¤ã€‚"
            elif len(source_docs) < 2:
                answer += "\n\nğŸ’¡ æç¤ºï¼šæ­¤å›ç­”åŸºäºå°‘é‡æ–‡æ¡£ä¿¡æ¯ï¼Œå¯èƒ½ä¸å¤Ÿå…¨é¢ã€‚"
            
            # æ·»åŠ ç›¸å…³æ–‡æ¡£æ•°é‡ä¿¡æ¯
            if source_docs:
                answer += f"\n\nğŸ“š å‚è€ƒäº† {len(source_docs)} ä¸ªç›¸å…³æ–‡æ¡£ç‰‡æ®µ"
            
            return answer
            
        except Exception as e:
            logger.error(f"åå¤„ç†ç­”æ¡ˆå¤±è´¥: {e}")
            return answer
    
    def convert_source_documents(self, source_docs: List[Document]) -> List[SourceDocument]:
        """è½¬æ¢æºæ–‡æ¡£æ ¼å¼å¹¶å»é‡"""
        converted_docs = []
        seen_docs = set()  # ç”¨äºå»é‡çš„é›†åˆ
        
        for doc in source_docs:
            try:
                # å…ˆå¯¹ metadata åšå®‰å…¨åºåˆ—åŒ–å¤„ç†ï¼Œé¿å… numpyã€UUID ç­‰ç±»å‹å¯¼è‡´ JSON ç¼–ç é”™è¯¯
                sanitized_metadata = DocumentProcessor._sanitize_metadata(doc.metadata)
                
                # ç”Ÿæˆæ–‡æ¡£å”¯ä¸€æ ‡è¯†ç”¨äºå»é‡
                doc_id = self._get_document_unique_id(doc, sanitized_metadata)
                
                # å¦‚æœå·²ç»å­˜åœ¨ç›¸åŒçš„æ–‡æ¡£ï¼Œè·³è¿‡
                if doc_id in seen_docs:
                    logger.debug(f"è·³è¿‡é‡å¤æ–‡æ¡£: {doc_id}")
                    continue
                
                seen_docs.add(doc_id)
                
                # æå–å¹¶è½¬æ¢ç›¸ä¼¼åº¦åˆ†æ•°ï¼Œç¡®ä¿ä¸º float åŸºç¡€ç±»å‹
                raw_score = sanitized_metadata.get('score', 0.0)
                try:
                    similarity_score = float(raw_score)
                except Exception:
                    similarity_score = 0.0
                
                # æå–æ–‡æ¡£æ ‡é¢˜å’Œæ¥æºä¿¡æ¯ï¼ˆä¼˜å…ˆä½¿ç”¨æ›´èƒ½åŒºåˆ†çš„å­—æ®µï¼‰
                display_title = (
                    sanitized_metadata.get('document_title')
                    or sanitized_metadata.get('filename')
                    or sanitized_metadata.get('title')
                    or 'æœªçŸ¥æ–‡æ¡£'
                )
                # Excel/å¤šè¡¨æ ¼ç±»æ–‡æ¡£ï¼Œé™„åŠ å·¥ä½œè¡¨ä¸æ‰¹æ¬¡ä¿¡æ¯ï¼Œé¿å…æ ‡é¢˜é‡å¤
                sheet_name = sanitized_metadata.get('sheet_name')
                if sheet_name:
                    display_title = f"{display_title} - {sheet_name}"
                if sanitized_metadata.get('batch_type') == 'data':
                    bs = sanitized_metadata.get('batch_start')
                    be = sanitized_metadata.get('batch_end')
                    if bs is not None and be is not None:
                        display_title = f"{display_title} [{bs}-{be}]"
                
                source = (
                    sanitized_metadata.get('source')
                    or sanitized_metadata.get('filename')
                    or 'æœªçŸ¥æ¥æº'
                )
                page_no = sanitized_metadata.get('page_number')
                if page_no is None:
                    page_no = sanitized_metadata.get('page')
                
                source_doc = SourceDocument(
                    content=doc.page_content[:500] + "..." if len(doc.page_content) > 500 else doc.page_content,
                    source=source,
                    title=display_title,
                    metadata=sanitized_metadata,
                    similarity_score=similarity_score,
                    document_id=sanitized_metadata.get('document_id'),
                    page_number=page_no,
                    kb_id=sanitized_metadata.get('kb_id'),
                    kb_name=sanitized_metadata.get('kb_name')
                )
                converted_docs.append(source_doc)
            except Exception as e:
                logger.error(f"è½¬æ¢æºæ–‡æ¡£å¤±è´¥: {e}")
                continue
        
        logger.info(f"æºæ–‡æ¡£å»é‡å®Œæˆ: åŸå§‹{len(source_docs)}ä¸ªï¼Œå»é‡å{len(converted_docs)}ä¸ª")
        return converted_docs

    def _get_document_unique_id(self, doc: Document, metadata: Dict[str, Any]) -> str:
        """ç”Ÿæˆæ–‡æ¡£å”¯ä¸€æ ‡è¯†ç”¨äºå»é‡"""
        try:
            # ä¼˜å…ˆä½¿ç”¨chunk_idï¼ˆæœ€ç²¾ç¡®çš„æ ‡è¯†ï¼‰
            if 'chunk_id' in metadata:
                return f"chunk_{metadata['chunk_id']}"
            
            # å…¶æ¬¡ä½¿ç”¨document_id + chunk_indexç»„åˆ
            if 'document_id' in metadata and 'chunk_index' in metadata:
                return f"doc_{metadata['document_id']}_chunk_{metadata['chunk_index']}"
            
            # ä½¿ç”¨document_id + page_numberç»„åˆ
            if 'document_id' in metadata and 'page_number' in metadata:
                return f"doc_{metadata['document_id']}_page_{metadata['page_number']}"
            
            # ä½¿ç”¨source + title + page_numberç»„åˆ
            if 'source' in metadata and 'title' in metadata and 'page_number' in metadata:
                return f"source_{hash(metadata['source'])}_{hash(metadata['title'])}_page_{metadata['page_number']}"
            
            # æœ€åä½¿ç”¨å†…å®¹å“ˆå¸Œï¼ˆç¡®ä¿å”¯ä¸€æ€§ï¼‰
            import hashlib
            content_hash = hashlib.md5(doc.page_content.encode('utf-8')).hexdigest()[:16]
            return f"content_{content_hash}"
            
        except Exception as e:
            logger.warning(f"ç”Ÿæˆæ–‡æ¡£å”¯ä¸€æ ‡è¯†å¤±è´¥: {e}")
            # é™çº§åˆ°å¯¹è±¡ID
            return f"fallback_{id(doc)}"

    def create_answer_response(self, 
                              question: str,
                              answer: str,
                              source_documents: List[SourceDocument],
                              processing_time: float,
                              from_cache: bool = False,
                              session_id: Optional[str] = None,
                              metadata: Optional[Dict[str, Any]] = None,
                              qa_log_id: Optional[uuid.UUID] = None) -> AnswerResponse:
        """åˆ›å»ºç­”æ¡ˆå“åº”"""
        formatted_answer = FormattedAnswer(
            summary=answer,
            steps=[],
            warnings=[],
            references=[]
        )
        
        # è®¡ç®—ç½®ä¿¡åº¦
        confidence = self.calculate_confidence_score(answer, source_documents)
        
        return AnswerResponse(
            question=question,
            answer=answer,
            formatted_answer=formatted_answer,
            source_documents=source_documents,
            confidence=confidence,
            processing_time=processing_time,
            from_cache=from_cache,
            session_id=session_id,
            metadata=metadata or {},
            created_at=datetime.now(timezone.utc),
            qa_log_id=qa_log_id
        )
    
    def create_error_response(self, error_message: str) -> AnswerResponse:
        """åˆ›å»ºé”™è¯¯å“åº”"""
        formatted_answer = FormattedAnswer(
            summary=f"æŠ±æ­‰ï¼Œ{error_message}",
            steps=[],
            warnings=[],
            references=[]
        )
        
        return AnswerResponse(
            question="",
            answer=error_message,
            formatted_answer=formatted_answer,
            source_documents=[],
            processing_time=0.0,
            created_at=datetime.now(timezone.utc)
        )
    
    def calculate_confidence_score(self, 
                                  answer: str, 
                                  source_documents: List[SourceDocument]) -> float:
        """è®¡ç®—ç½®ä¿¡åº¦åˆ†æ•°"""
        try:
            # åŸºç¡€åˆ†æ•°
            base_score = 0.5
            
            # æ ¹æ®æºæ–‡æ¡£æ•°é‡è°ƒæ•´
            doc_count_bonus = min(len(source_documents) * 0.1, 0.3)
            
            # æ ¹æ®ç­”æ¡ˆé•¿åº¦è°ƒæ•´
            length_bonus = min(len(answer) / 1000, 0.1)
            
            # æ ¹æ®å…³é”®è¯åŒ¹é…è°ƒæ•´
            keyword_bonus = 0.0
            general_keywords = ["æ­¥éª¤", "æµç¨‹", "æ–¹æ³•", "æ³¨æ„", "è¦æ±‚", "è¯´æ˜", "ä»‹ç»"]
            for keyword in general_keywords:
                if keyword in answer:
                    keyword_bonus += 0.02
            
            # è®¡ç®—æœ€ç»ˆåˆ†æ•°
            confidence_score = base_score + doc_count_bonus + length_bonus + keyword_bonus
            
            # é™åˆ¶åœ¨0-1èŒƒå›´å†…
            return min(max(confidence_score, 0.0), 1.0)
            
        except Exception as e:
            logger.error(f"è®¡ç®—ç½®ä¿¡åº¦å¤±è´¥: {e}")
            return 0.5
    
    def log_qa_interaction(self, 
                          db: Session,
                          question: str,
                          answer: str,
                          source_documents: List[SourceDocument],
                          user_id: Optional[int] = None,
                          session_id: Optional[str] = None,
                          processing_time: float = 0.0) -> Optional[QALog]:
        """è®°å½•é—®ç­”äº¤äº’"""
        try:
            # åºåˆ—åŒ–source_documentsï¼Œç¡®ä¿UUIDå¯¹è±¡è¢«è½¬æ¢ä¸ºå­—ç¬¦ä¸²
            serialized_docs = []
            for doc in source_documents:
                doc_dict = doc.dict()
                # ç¡®ä¿document_idè¢«è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                if 'document_id' in doc_dict and doc_dict['document_id'] is not None:
                    doc_dict['document_id'] = str(doc_dict['document_id'])
                # ç¡®ä¿metadataä¸­çš„UUIDä¹Ÿè¢«è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                if 'metadata' in doc_dict and isinstance(doc_dict['metadata'], dict):
                    for key, value in doc_dict['metadata'].items():
                        if hasattr(value, '__str__') and 'UUID' in str(type(value)):
                            doc_dict['metadata'][key] = str(value)
                serialized_docs.append(doc_dict)
            
            qa_log = QALog(
                question=question,
                answer=answer,
                user_id=user_id,
                session_id=session_id,
                retrieved_documents=serialized_docs,
                response_time=processing_time,
                log_metadata={
                    'model': settings.llm_model,
                    'temperature': settings.llm_temperature,
                    'retrieval_k': settings.retrieval_k,
                    'timestamp': datetime.now(timezone.utc).isoformat()
                }
            )
            
            db.add(qa_log)
            logger.info("QAæ—¥å¿—å·²æ·»åŠ åˆ°ä¼šè¯ï¼Œå‡†å¤‡æäº¤")
            
            db.commit()
            logger.info("QAæ—¥å¿—å·²æäº¤åˆ°æ•°æ®åº“")
            
            db.refresh(qa_log)
            logger.info(f"é—®ç­”æ—¥å¿—è®°å½•æˆåŠŸ: {qa_log.id}")
            return qa_log
            
        except Exception as e:
            logger.error(f"è®°å½•é—®ç­”æ—¥å¿—å¤±è´¥: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            db.rollback()
            return None
    
    def get_qa_history(self, 
                      db: Session,
                      user_id: Optional[int] = None,
                      session_id: Optional[str] = None,
                      limit: int = 20) -> List[QALogResponse]:
        """è·å–é—®ç­”å†å²"""
        try:
            query = db.query(QALog)
            
            if user_id:
                query = query.filter(QALog.user_id == user_id)
            
            if session_id:
                query = query.filter(QALog.session_id == session_id)
            
            qa_logs = query.order_by(QALog.created_at.desc()).limit(limit).all()
            
            return [
                QALogResponse(
                    id=log.id,
                    question=log.question,
                    answer=log.answer,
                    user_id=log.user_id,
                    session_id=log.session_id,
                    formatted_answer=log.formatted_answer,
                    retrieved_documents=log.retrieved_documents,
                    response_time=log.response_time,
                    satisfaction_score=log.satisfaction_score or 0,
                    feedback=log.feedback,
                    is_helpful=log.is_helpful,
                    created_at=log.created_at
                )
                for log in qa_logs
            ]
            
        except Exception as e:
            logger.error(f"è·å–é—®ç­”å†å²å¤±è´¥: {e}")
            return []
    
    def submit_feedback(self, 
                       db: Session,
                       qa_log_id: uuid.UUID,
                       score: int,
                       comment: Optional[str] = None,
                       is_helpful: Optional[bool] = None) -> bool:
        """æäº¤åé¦ˆ"""
        try:
            qa_log = db.query(QALog).filter(QALog.id == qa_log_id).first()
            if not qa_log:
                return False
            
            qa_log.satisfaction_score = score
            qa_log.feedback = comment
            if is_helpful is not None:
                qa_log.is_helpful = is_helpful
            
            db.commit()
            
            logger.info(f"åé¦ˆæäº¤æˆåŠŸ: QAæ—¥å¿— {qa_log_id}, è¯„åˆ† {score}")
            return True
            
        except Exception as e:
            logger.error(f"æäº¤åé¦ˆå¤±è´¥: {e}")
            db.rollback()
            return False
    
    # ç¼“å­˜ç›¸å…³æ–¹æ³•
    def cache_answer(self, 
                    question: str, 
                    category: Optional[str],
                    answer: str, 
                    source_documents: List[SourceDocument]):
        """ç¼“å­˜ç­”æ¡ˆ"""
        try:
            redis_client = self.get_redis_client()
            if redis_client:
                cache_key = self.get_cache_key(question, category)
                cache_data = {
                    'answer': answer,
                    'sources': [doc.dict() for doc in source_documents],
                    'timestamp': datetime.now(timezone.utc).isoformat()
                }
                
                redis_client.setex(
                    cache_key, 
                    settings.answer_cache_ttl, 
                    json.dumps(cache_data, ensure_ascii=False)
                )
                
        except Exception as e:
            logger.warning(f"ç¼“å­˜ç­”æ¡ˆå¤±è´¥: {e}")
    
    def get_cached_answer(self, 
                         question: str, 
                         category: Optional[str]) -> Optional[Dict[str, Any]]:
        """è·å–ç¼“å­˜çš„ç­”æ¡ˆ"""
        try:
            redis_client = self.get_redis_client()
            if redis_client:
                cache_key = self.get_cache_key(question, category)
                cached_data = redis_client.get(cache_key)
                
                if cached_data:
                    data = json.loads(cached_data.decode())
                    # è½¬æ¢æºæ–‡æ¡£
                    data['sources'] = [
                        SourceDocument(**doc) for doc in data['sources']
                    ]
                    return data
            
            return None
            
        except Exception as e:
            logger.warning(f"è·å–ç¼“å­˜ç­”æ¡ˆå¤±è´¥: {e}")
            return None
    
    def get_cache_key(self, question: str, category: Optional[str]) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        import hashlib
        
        key_parts = [question.lower().strip()]
        if category:
            key_parts.append(category.lower())
        
        key_string = "|".join(key_parts)
        key_hash = hashlib.md5(key_string.encode()).hexdigest()
        
        return f"qa_answer:{key_hash}"
    
    def clear_answer_cache(self, pattern: str = "qa_answer:*") -> int:
        """æ¸…é™¤ç­”æ¡ˆç¼“å­˜"""
        try:
            redis_client = self.get_redis_client()
            if redis_client:
                keys = redis_client.keys(pattern)
                if keys:
                    deleted_count = redis_client.delete(*keys)
                    logger.info(f"æ¸…é™¤äº† {deleted_count} ä¸ªç¼“å­˜ç­”æ¡ˆ")
                    return deleted_count
            return 0
            
        except Exception as e:
            logger.error(f"æ¸…é™¤ç­”æ¡ˆç¼“å­˜å¤±è´¥: {e}")
            return 0
    
    def get_qa_statistics(self, db: Session) -> Dict[str, Any]:
        """è·å–é—®ç­”ç»Ÿè®¡ä¿¡æ¯"""
        try:
            from sqlalchemy import func, desc
            
            # åŸºæœ¬ç»Ÿè®¡
            total_questions = db.query(QALog).count()
            
            # ä»Šæ—¥é—®ç­”æ•°
            today = datetime.now(timezone.utc).date()
            today_questions = db.query(QALog).filter(
                func.date(QALog.created_at) == today
            ).count()
            
            # å¹³å‡å¤„ç†æ—¶é—´
            avg_processing_time = db.query(
                func.avg(QALog.response_time)
            ).scalar() or 0.0
            
            # åé¦ˆç»Ÿè®¡
            feedback_stats = db.query(
                QALog.satisfaction_score,
                func.count(QALog.id).label('count')
            ).filter(
                QALog.satisfaction_score.isnot(None)
            ).group_by(QALog.satisfaction_score).all()
            
            feedback_distribution = {score: count for score, count in feedback_stats}
            
            # çƒ­é—¨é—®é¢˜
            popular_questions = db.query(
                QALog.question,
                func.count(QALog.id).label('count')
            ).group_by(
                QALog.question
            ).order_by(
                desc('count')
            ).limit(10).all()
            
            return {
                'total_questions': total_questions,
                'today_questions': today_questions,
                'average_processing_time': round(avg_processing_time, 2),
                'feedback_distribution': feedback_distribution,
                'popular_questions': [
                    {'question': q, 'count': c} for q, c in popular_questions
                ],
                'last_updated': datetime.now(timezone.utc).isoformat()
            }
            
        except Exception as e:
            logger.error(f"è·å–é—®ç­”ç»Ÿè®¡å¤±è´¥: {e}")
            return {}
    
    def add_documents_to_advanced_retriever(self, documents: List[Document]):
        """å‘é«˜çº§æ£€ç´¢å™¨æ·»åŠ æ–‡æ¡£"""
        if self.advanced_retriever:
            try:
                self.advanced_retriever.add_documents_to_parent_retriever(documents)
                logger.info(f"æˆåŠŸå‘é«˜çº§æ£€ç´¢å™¨æ·»åŠ  {len(documents)} ä¸ªæ–‡æ¡£")
            except Exception as e:
                logger.error(f"å‘é«˜çº§æ£€ç´¢å™¨æ·»åŠ æ–‡æ¡£å¤±è´¥: {e}")
    
    def get_advanced_retriever_stats(self) -> Dict[str, Any]:
        """è·å–é«˜çº§æ£€ç´¢å™¨ç»Ÿè®¡ä¿¡æ¯"""
        if self.advanced_retriever:
            return self.advanced_retriever.get_retriever_stats()
        return {"advanced_retriever_enabled": False}
    
    def _is_stakeholder_question(self, question: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºç›¸å…³æ–¹é—®é¢˜"""
        stakeholder_keywords = [
            "ç›¸å…³æ–¹", "åˆ©ç›Šç›¸å…³æ–¹", "stakeholder", "ç›¸å…³è€…",
            "åˆ©ç›Šç›¸å…³è€…", "å…³è”æ–¹", "ç›¸å…³äººå‘˜", "ç›¸å…³ç»„ç»‡"
        ]
        return any(keyword in question.lower() for keyword in stakeholder_keywords)
    
    def _handle_kimi_file_question(self, db: Session, question: str, 
                                  kimi_files: List[str], user_id: Optional[int], 
                                  session_id: Optional[str], start_time: datetime) -> AnswerResponse:
        """å¤„ç†Kimiæ¨¡å‹çš„æ–‡ä»¶é—®ç­”"""
        try:
            logger.info(f"ğŸ“ å¼€å§‹å¤„ç†Kimiæ–‡ä»¶é—®ç­”ï¼Œæ–‡ä»¶æ•°é‡: {len(kimi_files)}")
            
            # è·å–æ–‡ä»¶å†…å®¹
            from app.services.kimi_file_service import KimiFileService
            kimi_file_service = KimiFileService()
            
            file_contents = []
            for file_id in kimi_files:
                try:
                    content = kimi_file_service.get_file_content(file_id)
                    if content:
                        file_contents.append(content)
                        logger.info(f"ğŸ“„ æˆåŠŸè·å–æ–‡ä»¶å†…å®¹: {file_id}")
                except Exception as e:
                    logger.warning(f"âš ï¸ è·å–æ–‡ä»¶å†…å®¹å¤±è´¥: {file_id}, é”™è¯¯: {str(e)}")
            
            if not file_contents:
                logger.error("âŒ æœªèƒ½è·å–ä»»ä½•æ–‡ä»¶å†…å®¹")
                return self.create_error_response("æ— æ³•è·å–æ–‡ä»¶å†…å®¹ï¼Œè¯·é‡æ–°ä¸Šä¼ æ–‡ä»¶")
            
            # æ„å»ºåŒ…å«æ–‡ä»¶å†…å®¹çš„æç¤º
            file_context = "\n\n".join([f"æ–‡ä»¶å†…å®¹ {i+1}:\n{content}" for i, content in enumerate(file_contents)])
            enhanced_question = f"åŸºäºä»¥ä¸‹æ–‡ä»¶å†…å®¹å›ç­”é—®é¢˜ï¼š\n\n{file_context}\n\né—®é¢˜ï¼š{question}"
            
            logger.info(f"ğŸ“ æ„å»ºå¢å¼ºé—®é¢˜ï¼Œæ€»é•¿åº¦: {len(enhanced_question)} å­—ç¬¦")
            
            # ä½¿ç”¨LLMæœåŠ¡ç›´æ¥å›ç­”
            if self.llm_service:
                try:
                    answer = self.llm_service.generate_response(enhanced_question)
                    logger.info(f"âœ… Kimiæ¨¡å‹å›ç­”ç”ŸæˆæˆåŠŸï¼Œé•¿åº¦: {len(answer)} å­—ç¬¦")
                    
                    # è®¡ç®—å¤„ç†æ—¶é—´
                    processing_time = (datetime.now(timezone.utc) - start_time).total_seconds()
                    
                    # åˆ›å»ºæºæ–‡æ¡£ï¼ˆåŸºäºæ–‡ä»¶ï¼‰
                    source_documents = []
                    for i, file_id in enumerate(kimi_files):
                        source_documents.append(SourceDocument(
                            content=file_contents[i][:200] + "..." if len(file_contents[i]) > 200 else file_contents[i],
                            metadata={
                                "source": f"kimi_file_{file_id}",
                                "file_id": file_id,
                                "type": "kimi_file"
                            },
                            score=1.0
                        ))
                    
                    # è®°å½•é—®ç­”æ—¥å¿—
                    qa_log = self.log_qa_interaction(
                        db, question, answer, source_documents, 
                        user_id, session_id, processing_time
                    )
                    
                    return self.create_answer_response(
                        question=question,
                        answer=answer,
                        source_documents=source_documents,
                        processing_time=processing_time,
                        session_id=session_id,
                        metadata={"model": self.current_model, "kimi_files": kimi_files},
                        qa_log_id=qa_log.id if qa_log else None
                    )
                    
                except Exception as e:
                    logger.error(f"âŒ Kimiæ¨¡å‹å›ç­”ç”Ÿæˆå¤±è´¥: {str(e)}")
                    return self.create_error_response(f"ç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {str(e)}")
            else:
                logger.error("âŒ LLMæœåŠ¡æœªåˆå§‹åŒ–")
                return self.create_error_response("è¯­è¨€æ¨¡å‹æœåŠ¡ä¸å¯ç”¨")
                
        except Exception as e:
            logger.error(f"âŒ å¤„ç†Kimiæ–‡ä»¶é—®ç­”æ—¶å‡ºé”™: {str(e)}")
            return self.create_error_response(f"å¤„ç†æ–‡ä»¶é—®ç­”æ—¶å‡ºé”™: {str(e)}")
    
    def _handle_stakeholder_question(self, db: Session, question: str, 
                                   user_id: Optional[int], session_id: Optional[str], 
                                   start_time: datetime) -> AnswerResponse:
        """å¤„ç†ç›¸å…³æ–¹é—®é¢˜"""
        try:
            logger.info("ğŸ‘¥ å¼€å§‹å¤„ç†ç›¸å…³æ–¹é—®é¢˜")
            
            # ä½¿ç”¨å…³é”®è¯æœç´¢ç›¸å…³æ–¹ä¿¡æ¯
            stakeholder_keywords = ["ç›¸å…³æ–¹", "åˆ©ç›Šç›¸å…³æ–¹", "é¡¾å®¢", "ä¾›æ–¹", "å‘˜å·¥", "è‚¡ä¸œ"]
            logger.info(f"ğŸ”‘ æœç´¢å…³é”®è¯: {stakeholder_keywords}")
            
            all_docs = []
            for i, keyword in enumerate(stakeholder_keywords):
                logger.info(f"ğŸ” æ­¥éª¤{i+1}: æœç´¢å…³é”®è¯ '{keyword}'")
                # ä½¿ç”¨å‘é‡æœç´¢
                retriever = self.vector_service.vector_store.as_retriever(
                    search_type="similarity",
                    search_kwargs={"k": 10}
                )
                docs = retriever.get_relevant_documents(keyword)
                all_docs.extend(docs)
                logger.info(f"ğŸ“Š å…³é”®è¯ '{keyword}' æœç´¢ç»“æœ: {len(docs)} ä¸ªæ–‡æ¡£")
            
            logger.info(f"ğŸ“‹ åˆå¹¶å‰æ–‡æ¡£æ€»æ•°: {len(all_docs)}")
            
            # å»é‡
            logger.info("ğŸ”„ å¼€å§‹æ–‡æ¡£å»é‡å¤„ç†")
            unique_docs = []
            seen_content = set()
            for doc in all_docs:
                if doc.page_content not in seen_content:
                    unique_docs.append(doc)
                    seen_content.add(doc.page_content)
            
            logger.info(f"ğŸ“‹ å»é‡åæ–‡æ¡£æ•°é‡: {len(unique_docs)}")
            
            if not unique_docs:
                logger.warning("âŒ æœªæ‰¾åˆ°ä»»ä½•ç›¸å…³æ–¹æ–‡æ¡£")
                return self.create_error_response("æœªæ‰¾åˆ°ç›¸å…³æ–¹ä¿¡æ¯")
            
            # æ„å»ºä¸Šä¸‹æ–‡
            logger.info("ğŸ”„ æ„å»ºä¸Šä¸‹æ–‡")
            selected_docs = unique_docs[:15]
            context = "\n\n".join([doc.page_content for doc in selected_docs])
            context_length = len(context)
            logger.info(f"ğŸ“ ä¸Šä¸‹æ–‡æ„å»ºå®Œæˆ: ä½¿ç”¨ {len(selected_docs)} ä¸ªæ–‡æ¡£, æ€»é•¿åº¦ {context_length} å­—ç¬¦")
            
            # ä½¿ç”¨LLMç”Ÿæˆç­”æ¡ˆ
            logger.info("ğŸ¤– å¼€å§‹ä½¿ç”¨LLMç”Ÿæˆç­”æ¡ˆ")
            prompt = f"""
åŸºäºä»¥ä¸‹æ–‡æ¡£å†…å®¹ï¼Œè¯¦ç»†å›ç­”å…³äºç›¸å…³æ–¹çš„é—®é¢˜ã€‚è¯·åˆ—å‡ºæ‰€æœ‰ç›¸å…³æ–¹ç±»å‹å¹¶è¯´æ˜å…¶ç‰¹ç‚¹ã€‚

æ–‡æ¡£å†…å®¹ï¼š
{context}

é—®é¢˜ï¼š{question}

è¯·æä¾›è¯¦ç»†ã€å‡†ç¡®çš„ç­”æ¡ˆï¼š
"""
            
            logger.info(f"ğŸ“ æç¤ºè¯é•¿åº¦: {len(prompt)} å­—ç¬¦")
            response = self.llm.invoke(prompt)
            answer = response.content if hasattr(response, 'content') else str(response)
            logger.info(f"ğŸ“ LLMç­”æ¡ˆç”Ÿæˆå®Œæˆ: {len(answer)} å­—ç¬¦")
            
            # è½¬æ¢æºæ–‡æ¡£
            logger.info("ğŸ”„ è½¬æ¢æºæ–‡æ¡£æ ¼å¼")
            source_documents = self.convert_source_documents(unique_docs[:10])
            logger.info(f"ğŸ“„ æˆåŠŸè½¬æ¢ {len(source_documents)} ä¸ªæºæ–‡æ¡£")
            
            # è®¡ç®—å¤„ç†æ—¶é—´
            processing_time = (datetime.now(timezone.utc) - start_time).total_seconds()
            logger.info(f"â±ï¸ ç›¸å…³æ–¹é—®é¢˜å¤„ç†æ—¶é—´: {processing_time:.2f}ç§’")
            
            # ç¼“å­˜ç­”æ¡ˆ
            logger.info("ğŸ’¾ ç¼“å­˜ç›¸å…³æ–¹ç­”æ¡ˆ")
            self.cache_answer(question, "ç›¸å…³æ–¹", answer, source_documents)
            
            # è®°å½•é—®ç­”æ—¥å¿—
            logger.info("ğŸ“ è®°å½•ç›¸å…³æ–¹é—®ç­”æ—¥å¿—")
            qa_log = self.log_qa_interaction(
                db=db,
                question=question,
                answer=answer,
                source_documents=source_documents,
                user_id=user_id,
                session_id=session_id,
                processing_time=processing_time
            )
            
            logger.info("âœ… ç›¸å…³æ–¹é—®é¢˜å¤„ç†å®Œæˆ")
            return self.create_answer_response(
                question=question,
                answer=answer,
                source_documents=source_documents,
                processing_time=processing_time,
                session_id=session_id,
                metadata={
                    "retrieval_method": "stakeholder_special",
                    "category": "ç›¸å…³æ–¹"
                },
                qa_log_id=qa_log.id if qa_log else None
            )
            
        except Exception as e:
            processing_time = (datetime.now(timezone.utc) - start_time).total_seconds()
            logger.error(f"âŒ å¤„ç†ç›¸å…³æ–¹é—®é¢˜å¤±è´¥: {e} (è€—æ—¶: {processing_time:.2f}ç§’)")
            return self.create_error_response(f"å¤„ç†ç›¸å…³æ–¹é—®é¢˜å¤±è´¥: {str(e)}")

# QAæœåŠ¡å·¥å…·å‡½æ•°
def extract_keywords_from_question(question: str) -> List[str]:
    """ä»é—®é¢˜ä¸­æå–å…³é”®è¯"""
    import re
    from collections import Counter
    
    # ç®€å•çš„å…³é”®è¯æå–
    words = re.findall(r'[\u4e00-\u9fa5a-zA-Z]+', question)
    
    # è¿‡æ»¤åœç”¨è¯
    stop_words = {'æ€ä¹ˆ', 'å¦‚ä½•', 'ä»€ä¹ˆ', 'å“ªä¸ª', 'ä¸ºä»€ä¹ˆ', 'æ˜¯å¦', 'å¯ä»¥', 'éœ€è¦'}
    keywords = [word for word in words if word not in stop_words and len(word) > 1]
    
    # è¿”å›æœ€å¸¸è§çš„å…³é”®è¯
    word_counts = Counter(keywords)
    return [word for word, count in word_counts.most_common(5)]

def format_answer_with_steps(answer: str) -> str:
    """æ ¼å¼åŒ–åŒ…å«æ­¥éª¤çš„ç­”æ¡ˆ"""
    try:
        # æ£€æµ‹æ­¥éª¤æ¨¡å¼
        step_patterns = [
            r'(\d+[.ã€])',  # æ•°å­—æ­¥éª¤
            r'([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å][.ã€])',  # ä¸­æ–‡æ•°å­—æ­¥éª¤
            r'(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+æ­¥)',  # ç¬¬Xæ­¥
        ]
        
        formatted_answer = answer
        
        for pattern in step_patterns:
            if re.search(pattern, answer):
                # åœ¨æ­¥éª¤å‰æ·»åŠ æ¢è¡Œ
                formatted_answer = re.sub(
                    pattern, 
                    r'\n\1', 
                    formatted_answer
                )
                break
        
        return formatted_answer.strip()
        
    except Exception as e:
        logger.error(f"æ ¼å¼åŒ–ç­”æ¡ˆå¤±è´¥: {e}")
        return answer