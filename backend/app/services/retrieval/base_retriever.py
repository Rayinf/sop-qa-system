from abc import ABC, abstractmethod
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
from enum import Enum
import logging
from datetime import datetime, timezone

from langchain.schema import Document

logger = logging.getLogger(__name__)

class RetrievalStrategy(Enum):
    """æ£€ç´¢ç­–ç•¥æšä¸¾"""
    VECTOR = "vector"
    HYBRID = "hybrid"
    PARENT = "parent"
    MULTI_QUERY = "multi_query"
    COMPRESSION = "compression"
    ENSEMBLE = "ensemble"
    ROUTING = "routing"

@dataclass
class RetrievalResult:
    """æ£€ç´¢ç»“æœæ•°æ®ç±»"""
    documents: List[Document]
    scores: Optional[List[float]] = None
    metadata: Optional[Dict[str, Any]] = None
    processing_time: Optional[float] = None
    strategy_used: Optional[str] = None
    fallback_used: bool = False

@dataclass
class RetrievalConfig:
    """æ£€ç´¢é…ç½®åŸºç±»"""
    k: int = 10
    similarity_threshold: float = 0.3
    category: Optional[str] = None
    enable_cache: bool = True
    cache_ttl: int = 3600  # ç¼“å­˜æ—¶é—´ï¼ˆç§’ï¼‰

class BaseRetriever(ABC):
    """ç»Ÿä¸€æ£€ç´¢å™¨åŸºç±»"""
    
    def __init__(self, config: RetrievalConfig):
        self.config = config
        self.strategy = self._get_strategy()
        self._stats = {
            "total_queries": 0,
            "total_documents_retrieved": 0,
            "average_processing_time": 0.0,
            "cache_hits": 0,
            "cache_misses": 0,
            "last_query_time": None
        }
        self._cache = {} if config.enable_cache else None
        
    @abstractmethod
    def _get_strategy(self) -> RetrievalStrategy:
        """è·å–æ£€ç´¢ç­–ç•¥ç±»å‹"""
        pass
    
    @abstractmethod
    def _retrieve_documents(self, query: str, **kwargs) -> RetrievalResult:
        """å®é™…çš„æ–‡æ¡£æ£€ç´¢é€»è¾‘"""
        pass
    
    def retrieve(self, query: str, **kwargs) -> RetrievalResult:
        """æ£€ç´¢æ–‡æ¡£çš„ç»Ÿä¸€å…¥å£"""
        start_time = datetime.now(timezone.utc)
        
        try:
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            self._stats["total_queries"] += 1
            self._stats["last_query_time"] = start_time.isoformat()
            
            # æ£€æŸ¥ç¼“å­˜
            cache_key = self._get_cache_key(query, **kwargs)
            if self._cache and cache_key in self._cache:
                self._stats["cache_hits"] += 1
                cached_result = self._cache[cache_key]
                logger.info(f"ğŸ¯ ç¼“å­˜å‘½ä¸­: {self.strategy.value} - '{query[:50]}{'...' if len(query) > 50 else ''}'")
                return cached_result
            
            if self._cache:
                self._stats["cache_misses"] += 1
            
            # æ‰§è¡Œæ£€ç´¢
            logger.info(f"ğŸ” å¼€å§‹æ£€ç´¢: {self.strategy.value} - '{query[:50]}{'...' if len(query) > 50 else ''}'")
            result = self._retrieve_documents(query, **kwargs)
            
            # è®¡ç®—å¤„ç†æ—¶é—´
            processing_time = (datetime.now(timezone.utc) - start_time).total_seconds()
            result.processing_time = processing_time
            result.strategy_used = self.strategy.value
            
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            self._stats["total_documents_retrieved"] += len(result.documents)
            self._update_average_processing_time(processing_time)
            
            # ç¼“å­˜ç»“æœ
            if self._cache and cache_key:
                self._cache[cache_key] = result
                # ç®€å•çš„ç¼“å­˜æ¸…ç†ç­–ç•¥
                if len(self._cache) > 1000:
                    # åˆ é™¤æœ€æ—§çš„ä¸€åŠç¼“å­˜
                    keys_to_delete = list(self._cache.keys())[:500]
                    for key in keys_to_delete:
                        del self._cache[key]
            
            logger.info(f"âœ… æ£€ç´¢å®Œæˆ: {self.strategy.value} - æ‰¾åˆ° {len(result.documents)} ä¸ªæ–‡æ¡£ï¼Œè€—æ—¶ {processing_time:.3f}s")
            return result
            
        except Exception as e:
            logger.error(f"âŒ æ£€ç´¢å¤±è´¥: {self.strategy.value} - {e}")
            # è¿”å›ç©ºç»“æœè€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
            return RetrievalResult(
                documents=[],
                metadata={"error": str(e)},
                processing_time=(datetime.now(timezone.utc) - start_time).total_seconds(),
                strategy_used=self.strategy.value
            )
    
    def _get_cache_key(self, query: str, **kwargs) -> Optional[str]:
        """ç”Ÿæˆç¼“å­˜é”®"""
        if not self.config.enable_cache:
            return None
        
        # åˆ›å»ºåŸºäºæŸ¥è¯¢å’Œå‚æ•°çš„ç¼“å­˜é”®
        key_parts = [query, str(self.config.k), str(self.config.similarity_threshold)]
        if self.config.category:
            key_parts.append(self.config.category)
        
        # æ·»åŠ å…¶ä»–å…³é”®å‚æ•°
        for key, value in sorted(kwargs.items()):
            if key in ['k', 'category', 'filter_dict']:
                key_parts.append(f"{key}:{value}")
        
        return f"{self.strategy.value}:" + ":".join(key_parts)
    
    def _update_average_processing_time(self, processing_time: float):
        """æ›´æ–°å¹³å‡å¤„ç†æ—¶é—´"""
        current_avg = self._stats["average_processing_time"]
        total_queries = self._stats["total_queries"]
        
        if total_queries == 1:
            self._stats["average_processing_time"] = processing_time
        else:
            # è®¡ç®—ç§»åŠ¨å¹³å‡
            self._stats["average_processing_time"] = (
                (current_avg * (total_queries - 1) + processing_time) / total_queries
            )
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–æ£€ç´¢å™¨ç»Ÿè®¡ä¿¡æ¯"""
        stats = self._stats.copy()
        stats.update({
            "strategy": self.strategy.value,
            "config": {
                "k": self.config.k,
                "similarity_threshold": self.config.similarity_threshold,
                "category": self.config.category,
                "enable_cache": self.config.enable_cache
            },
            "cache_size": len(self._cache) if self._cache else 0
        })
        return stats
    
    def reset(self) -> None:
        """é‡ç½®æ£€ç´¢å™¨çŠ¶æ€"""
        self._stats = {
            "total_queries": 0,
            "total_documents_retrieved": 0,
            "average_processing_time": 0.0,
            "cache_hits": 0,
            "cache_misses": 0,
            "last_query_time": None
        }
        if self._cache:
            self._cache.clear()
        logger.info(f"ğŸ”„ æ£€ç´¢å™¨å·²é‡ç½®: {self.strategy.value}")
    
    def clear_cache(self) -> int:
        """æ¸…ç©ºç¼“å­˜"""
        if not self._cache:
            return 0
        
        cache_size = len(self._cache)
        self._cache.clear()
        logger.info(f"ğŸ—‘ï¸ ç¼“å­˜å·²æ¸…ç©º: {self.strategy.value} - æ¸…é™¤äº† {cache_size} ä¸ªç¼“å­˜é¡¹")
        return cache_size
    
    def warm_up(self, queries: List[str]) -> Dict[str, Any]:
        """é¢„çƒ­æ£€ç´¢å™¨"""
        logger.info(f"ğŸ”¥ å¼€å§‹é¢„çƒ­æ£€ç´¢å™¨: {self.strategy.value} - {len(queries)} ä¸ªæŸ¥è¯¢")
        
        start_time = datetime.now(timezone.utc)
        results = []
        
        for query in queries:
            try:
                result = self.retrieve(query)
                results.append({
                    "query": query,
                    "documents_count": len(result.documents),
                    "processing_time": result.processing_time,
                    "success": True
                })
            except Exception as e:
                results.append({
                    "query": query,
                    "error": str(e),
                    "success": False
                })
        
        total_time = (datetime.now(timezone.utc) - start_time).total_seconds()
        successful_queries = sum(1 for r in results if r["success"])
        
        warmup_stats = {
            "total_queries": len(queries),
            "successful_queries": successful_queries,
            "failed_queries": len(queries) - successful_queries,
            "total_time": total_time,
            "average_time_per_query": total_time / len(queries) if queries else 0,
            "results": results
        }
        
        logger.info(f"âœ… é¢„çƒ­å®Œæˆ: {self.strategy.value} - {successful_queries}/{len(queries)} æˆåŠŸï¼Œè€—æ—¶ {total_time:.3f}s")
        return warmup_stats